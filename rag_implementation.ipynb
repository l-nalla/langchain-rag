{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'llama3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why couldn\\'t the bicycle stand up by itself?\\n\\nBecause it was two-tired!\\n\\n(Sorry, I know it\\'s a bit of a \"dad\" joke)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(input=\"Tell a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(Wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that brought a smile to your face!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser\n",
    "\n",
    "chain.invoke(\"Tell a nice joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='THE BIG BOOK OF GENERATIVE AICONTENTSIntroduction  ............................................................................................................................................................................................................ 3\\nThe Path to Deploying Production-Quality GenAI Applications  ............................................................................................. 5 \\nStage 0: Foundation Models  ................................................................................................................................................................................................................................................................ 5 \\n Use Case: Introducing DBRX: A New State-of-the-Art Open LLM  ...................................................................................................................................................................... 5 \\nStage 1: Prompt Engineering  .............................................................................................................................................................................................................................................................. 19 \\n Use Case: Automated Analysis of Product Reviews Using Large Language Models  ......................................................................................................................... 20 \\nStage 2: Retrieval Augmented Generation  ............................................................................................................................................................................................................................ 25 \\n Use Case: Improve Your RAG Application Response Quality With Real-Time Structured Data  .................................................................................................. 27 \\nStage 3: Fine-Tuning a Foundation Model  .............................................................................................................................................................................................................................. 33 \\n Use Case: Creating a Bespoke LLM for AI-Generated Documentation  ........................................................................................................................................................ 34 \\n Use Case: Efficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models  ..................................................... 43 \\nStage 4: Pretraining  ................................................................................................................................................................................................................................................................................ 60 \\n Use Case: Training Stable Diffusion From Scratch for <$50K With MosaicML  ........................................................................................................................................ 62 \\n Use Case: Deep Dive: How We Trained Stable Diffusion for Less Than $50K  .......................................................................................................................................... 68 \\nStage 5: LLM Evaluation  ......................................................................................................................................................................................................................................................................... 81 \\n Use Case : Best Practices for LLM Evaluation of RAG Application  .................................................................................................................................................................... 82 \\n Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks  ............................................................................................. 98', metadata={'source': 'genai_notes.pdf', 'page': 1}),\n",
       " Document(page_content='Use Case: Offline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks  ............................................................................................. 98\\nSummary  ................................................................................................................................................................................................................ 117 \\nGenAI Training  ............................................................................................................................................................................................................................................................................................. 117 \\nAdditional Resources  ............................................................................................................................................................................................................................................................................. 1172', metadata={'source': 'genai_notes.pdf', 'page': 1}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIAchieving Production-Quality GenAI Requires New Tools and Skills\\nGenerative AI has opened new worlds of possibilities for businesses and is being emphatically embraced \\nacross organizations. According to a recent MIT Tech Review  report, all 600 CIOs surveyed stated they are \\nincreasing their investment in AI, and 71% are planning to build their own custom large language models (LLMs) \\nor other GenAI models. However, many organizations have found it challenging to deploy these applications at \\nproduction quality. To meet the standard of quality required for customer-facing applications, AI output must \\nbe accurate, governed and safe. \\nData Infrastructure Must Evolve to Support  \\nGenAI-Powered Applications\\nMaking the leap to generative AI is not just about deploying a chatbot; it requires a reshaping of the foundational \\naspects of data management. Central to this transformation is the emergence of data lakehouses  as the new \\n“modern data stack.” These advanced data architectures are essential to harnessing the full potential of GenAI, \\ndriving faster, more cost-effective and wider democratization of data and AI technologies. As businesses \\nincreasingly rely on GenAI-powered tools and applications for competitive advantage, the underlying data \\ninfrastructure must evolve to support these advanced technologies effectively and securely.\\nNo Matter Where You Are on Your Path to Deploying GenAI Applications, \\nthe Quality of Your Data Matters\\nBusinesses need to achieve production quality with their GenAI applications. Developers need rich tools for \\nunderstanding the quality of their data and model outputs, along with an underlying platform that lets them \\ncombine and optimize all aspects of the GenAI process. GenAI has many components such as data preparation, \\nretrieval models, language models (either SaaS or open source), ranking and post-processing pipelines, prompt \\nengineering, and training models on custom enterprise data.\\nTo help you overcome common enterprise challenges with building GenAI, we’ve compiled a collection of \\ntechnical content and code samples. We’ll start each section with a brief overview and then provide use cases \\nand example code for reference. Introduction3', metadata={'source': 'genai_notes.pdf', 'page': 2}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIIn this eBook, you’ll learn: \\n ■How to plan a path from basic to advanced GenAI applications, leveraging your organization’s data\\n ■How to use retrieval augmented generation (RAG) to make an off-the-shelf AI system smarter\\n ■How to evaluate LLMs and where you want to invest in more powerful AI tools and systems that drive \\nmore significant operational gain\\n ■How to build a custom LLM that may be better, faster and cheaper for your organization\\n ■When it might be worth it to pretrain your own model — and more\\nUse cases for GenAI covered:\\n ■How to use LLMs to gain actionable insights from product reviews\\n ■How to use RAG for a chatbot to improve the quality of output\\n ■How to train your own generative AI model in a cost-effective manner\\n ■How to monitor and evaluate your deployed LLMs and GenAI applications\\n4', metadata={'source': 'genai_notes.pdf', 'page': 3}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe Path to Deploying \\nProduction-Quality \\nGenAI ApplicationsStage 0: Foundation Models\\nBefore setting off to create production-quality GenAI applications, we need to cover the base language models \\nthat serve as the foundation for layers of increasingly complex techniques. Foundation models commonly refer \\nto large language models that have been trained over extensive datasets to be generally good at some task \\n(chat, instruction following, code generation, etc.).\\nWe won’t cover many models, as it is a constantly shifting landscape, but it is important to note that while \\nunderlying architectures may differ drastically, foundation models generally fall under two categories: \\nproprietary (such as GPT-3.5 and Gemini) and open source (such as Llama2-70B and DBRX). The main difference \\nbetween the two is that while proprietary models historically have an edge on outright performance, users have \\nto send their data out to a third party and don’t have control over the underlying model as they’re often being \\nupdated and changed. \\nOpen source models, on the other hand, offer users full control over the model and the ability to run it on their \\nown terms with their own governance and data privacy. Here’s a current list of many open source GenAI models  \\nacross different domains that are all free for commercial use. Databricks has also created their own state-of-\\nthe-art open source foundation model so users can build the highest-quality production GenAI applications.\\nFoundation Model Use Case\\nINTRODUCING DBRX: A NEW STATE-OF-THE-ART OPEN LLM\\nWe are excited to introduce DBRX, an open, general-purpose LLM created by Databricks. Across a range of \\nstandard benchmarks, DBRX sets a new state-of-the-art for established open LLMs. Moreover, it provides \\nthe open community and enterprises building their own LLMs with capabilities that were previously limited \\nto closed model APIs; according to our measurements, it surpasses GPT-3.5, and it is competitive with \\nGemini 1.0 Pro. It is an especially capable code model, surpassing specialized models like CodeLLaMA-70B on \\nprogramming, in addition to its strength as a general-purpose LLM.5', metadata={'source': 'genai_notes.pdf', 'page': 4}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThis state-of-the-art quality comes with marked improvements in training and inference performance. DBRX \\nadvances the state-of-the-art in efficiency among open models thanks to its fine-grained mixture-of-experts \\n(MoE) architecture. Inference is up to 2x faster than LLaMA2-70B, and DBRX is about 40% of the size of Grok-1 in \\nterms of both total and active parameter-counts. When hosted on Mosaic AI Model Serving, DBRX can generate \\ntext at up to 150 tok/s/user. Our customers will find that training MoEs is also about 2x more FLOP-efficient \\nthan training dense models for the same final model quality. End-to-end, our overall recipe for DBRX (including \\nthe pretraining data, model architecture, and optimization strategy) can match the quality of our previous-\\ngeneration MPT models with nearly 4x less compute.\\nFigure 1: DBRX outperforms established open source models on language understanding (MMLU), \\nProgramming (HumanEval), and Math (GSM8K).\\n6', metadata={'source': 'genai_notes.pdf', 'page': 5}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe weights of the base model ( DBRX Base ) and the fine-tuned model ( DBRX Instruct ) are available on Hugging \\nFace under an open license. Starting today, DBRX is available for Databricks customers to use via APIs, and \\nDatabricks customers can pretrain their own DBRX-class models from scratch or continue training on top of  \\none of our checkpoints using the same tools and science we used to build it. DBRX is already being integrated \\ninto our GenAI-powered products, where — in applications like SQL — early rollouts have surpassed GPT-3.5 \\nTurbo and are challenging GPT-4 Turbo. It is also a leading model among open models and GPT-3.5 Turbo on \\nRAG tasks.\\nTraining mixture-of-experts models is hard. We had to overcome a variety of scientific and performance \\nchallenges to build a pipeline robust enough to repeatedly train DBRX-class models in an efficient manner. Now \\nthat we have done so, we have a one-of-a-kind training stack that allows any enterprise to train world-class MoE \\nfoundation models from scratch. We look forward to sharing that capability with our customers and sharing our \\nlessons learned with the community.\\nDownload DBRX today from Hugging Face ( DBRX Base , DBRX Instruct ), or try out DBRX Instruct in our HF Space , \\nor see our model repository on github: databricks/dbrx .\\nWhat Is DBRX?\\nDBRX is a transformer-based decoder-only large language model (LLM) that was trained using next-token \\nprediction. It uses a fine-grained mixture-of-experts (MoE) architecture with 132B total parameters of which \\n36B parameters are active on any input. It was pre-trained on 12T tokens of text and code data. Compared \\nto other open MoE models like Mixtral and Grok-1, DBRX is fine-grained, meaning it uses a larger number of \\nsmaller experts. DBRX has 16 experts and chooses 4, while Mixtral and Grok-1 have 8 experts and choose 2. This \\nprovides 65x more possible combinations of experts and we found that this improves model quality. DBRX uses \\nrotary position encodings (RoPE), gated linear units (GLU), and grouped query attention (GQA). It uses the GPT-\\n4 tokenizer as provided in the tiktoken  repository. We made these choices based on exhaustive evaluation and \\nscaling experiments.7', metadata={'source': 'genai_notes.pdf', 'page': 6}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIDBRX was pretrained on 12T tokens of carefully curated data and a maximum context length of 32k tokens. We \\nestimate that this data is at least 2x better token-for-token than the data we used to pretrain the MPT family of \\nmodels. This new dataset was developed using the full suite of Databricks tools, including Apache Spark™ and \\nDatabricks notebooks for data processing, Unity Catalog  for data management and governance, and MLflow for \\nexperiment tracking. We used curriculum learning for pretraining, changing the data mix during training in ways \\nwe found to substantially improve model quality.\\nQuality on Benchmarks vs. Leading Open Models\\nTable 1 shows the quality of DBRX Instruct and leading established, open models. DBRX Instruct is the leading \\nmodel on composite benchmarks, programming and mathematics benchmarks, and MMLU. It surpasses all chat \\nor instruction fine-tuned models on standard benchmarks.\\nComposite benchmarks. We evaluated DBRX Instruct and peers on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard  (the average of ARC-Challenge, HellaSwag, MMLU, TruthfulQA, WinoGrande,  \\nand GSM8k) and the Databricks Model Gauntlet  (a suite of over 30 tasks spanning six categories: world \\nknowledge, commonsense reasoning, language understanding, reading comprehension, symbolic problem \\nsolving, and programming).\\nAmong the models we evaluated, DBRX Instruct scores the highest on two composite benchmarks: the Hugging \\nFace Open LLM Leaderboard (74.5% vs. 72.7% for the next highest model, Mixtral Instruct) and the Databricks \\nGauntlet (66.8% vs. 60.7% for the next highest model, Mixtral Instruct).\\nProgramming and mathematics. DBRX Instruct is especially strong at programming and mathematics. It scores \\nhigher than the other open models we evaluated on HumanEval (70.1% vs. 63.2% for Grok-1, 54.8% for Mixtral \\nInstruct, and 32.2% for the best-performing LLaMA2-70B variant) and GSM8k (66.9% vs. 62.9% for Grok-1, 61.1% \\nfor Mixtral Instruct, and 54.1% for the best-performing LLaMA2-70B variant). DBRX outperforms Grok-1, the next \\nbest model on these benchmarks, despite the fact that Grok-1 has 2.4x as many parameters. On HumanEval, \\nDBRX Instruct even surpasses CodeLLaMA-70B Instruct, a model built explicitly for programming, despite the \\nfact that DBRX Instruct is designed for general-purpose use (70.1% vs. 67.8% on HumanEval as reported by Meta \\nin the CodeLLaMA blog ).\\nMMLU. DBRX Instruct scores higher than all other models we consider on MMLU, reaching 73.7%.8', metadata={'source': 'genai_notes.pdf', 'page': 7}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIMODELDBRX  \\nINSTRUCTMIXTRAL \\nINSTRUCTMIXTRAL \\nBASELLAMA2-70  \\nB CHATLLAMA2-70  \\nB BASEGROK-11\\nOpen LLM Leaderboard2  \\n(Avg of next 6 rows)74.5% 72.7% 68.4% 62.4% 67.9% —\\nARC-challenge 25-shot 68.9% 70.1% 66.4% 64.6% 67.3% —\\nHellaSwag 10-shot 89.0% 87.6% 86.5% 85.9% 87.3% —\\nMMLU 5-shot 73.7% 71.4% 71.9% 63.9% 69.8% 73.0%\\nTruthful QA 0-shot 66.9% 65.0% 46.8% 52.8% 44.9% —\\nWinoGrande 5-shot 81.8% 81.1% 81.7% 80.5% 83.7% —\\nGSM8k CoT 5-shot \\nmaj@1366.9% 61.1% 57.6% 26.7% 54.1%62.9%  \\n(8-shot)\\nGauntlet v0.34  \\n(Avg of 30+ diverse tasks)66.8% 60.7% 56.8% 52.8% 56.4% —\\nHumanEval5  \\n0-Shot, pass@1  \\n(Programming)70.1% 54.8% 40.2% 32.2% 31.0% 63.2%LLaMA2-70B Base\\nTable 1: Quality of DBRX Instruct and leading open models. See footnotes for details on how numbers were collected. \\nBolded and underlined is the highest score.9', metadata={'source': 'genai_notes.pdf', 'page': 8}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIQuality on Benchmarks vs. Leading Closed Models\\nTable 2 shows the quality of DBRX Instruct and leading closed models. According to the scores reported by \\neach model creator, DBRX Instruct surpasses GPT-3.5 (as described in the GPT-4 paper), and it is competitive \\nwith Gemini 1.0 Pro and Mistral Medium.\\nAcross nearly all benchmarks we considered, DBRX Instruct surpasses or - at worst - matches GPT-3.5. DBRX \\nInstruct outperforms GPT-3.5 on general knowledge as measured by MMLU (73.7% vs. 70.0%) and commonsense \\nreasoning as measured by HellaSwag (89.0% vs. 85.5%) and WinoGrande (81.8% vs. 81.6%). DBRX Instruct \\nespecially shines on programming and mathematical reasoning as measured by HumanEval (70.1% vs. 48.1%) and \\nGSM8k (72.8% vs. 57.1%).\\nDBRX Instruct is competitive with Gemini 1.0 Pro and Mistral Medium. Scores for DBRX Instruct are higher than \\nGemini 1.0 Pro on Inflection Corrected MTBench, MMLU, HellaSwag, and HumanEval, while Gemini 1.0 Pro is \\nstronger on GSM8k. Scores for DBRX Instruct and Mistral Medium are similar for HellaSwag, while Mistral Medium \\nis stronger on Winogrande and MMLU and DBRX Instruct is stronger on HumanEval, GSM8k, and Inflection \\nCorrected MTBench.10', metadata={'source': 'genai_notes.pdf', 'page': 9}),\n",
       " Document(page_content='MODELDBRX  \\nINSTRUCTGPT-3.57 GPT-48CLAUDE  \\n3 HAIKUCLAUDE 3 \\nSONNETCLAUDE 3 \\nOPUSGEMINI  \\n1.0 PROGEMINI  \\n1.5 PROMISTRAL  \\nMEDIUMMISTRAL \\nLARGE\\nMT Bench  \\n(Inflection corrected , n=5)8.39 ± 0.08 — —8.41 ± \\n0.04 8.54 ± \\n0.099.03 ± \\n0.068.23 ± 0.08 — 8.05 ± 0.128.90 ± \\n0.06\\nMMLU 5-shot 73.7% 70.0% 86.4% 75.2% 79.0% 86.8% 71.8% 81.9% 75.3% 81.2%\\nHellaSwag 10-shot 89.0% 85.5% 95.3% 85.9% 89.0% 95.4% 84.7% 92.5% 88.0% 89.2%\\nHumanEval 0-Shot  \\npass@1  \\n(Programming)70.1%  \\ntemp=0, \\nN=148.1% 67.0% 75.9% 73.0% 84.9% 67.7% 71.9% 38.4% 45.1%\\nGSM8k CoT maj@172.8%  \\n(5-shot)57.1%  \\n(5-shot)92.0%  \\n(5-shot)88.9% 92.3% 95.0%86.5%  \\n(maj1@32)91.7%  \\n(11-shot)66.7%  \\n(5-shot)81.0%  \\n(5-shot)\\nWinoGrande 5-shot 81.8% 81.6% 87.5% — — — — — 88.0% 86.7%\\nTable 2: Quality of DBRX Instruct and leading closed models. Other than Inflection Corrected MTBench (which we measured ourselves on model \\nendpoints), numbers were as reported by the creators of these models in their respective whitepapers. See footnotes for additional details.11\\nTHE BIG BOOK OF GENERATIVE AI', metadata={'source': 'genai_notes.pdf', 'page': 10}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIQuality on Long-Context Tasks and RAG\\nDBRX Instruct was trained with up to a 32K token context window. Table 3 compares its performance to that of \\nMixtral Instruct and the latest versions of the GPT-3.5 Turbo and GPT-4 Turbo APIs on a suite of long-context \\nbenchmarks (KV-Pairs from the Lost in the Middle  paper and HotpotQAXL, a modified version of HotPotQA that \\nextends the task to longer sequence lengths). GPT-4 Turbo is generally the best model at these tasks. However, \\nwith one exception, DBRX Instruct performs better than GPT-3.5 Turbo at all context lengths and all parts of the \\nsequence. Overall performance for DBRX Instruct and Mixtral Instruct are similar.\\nMODELDBRX  \\nINSTRUCTMIXTRAL  \\nINSTRUCTGPT-3.5 TURBO \\n(API)GPT-4 TURBO \\n(API)\\nAnswer in Beginning Third of Context 45.1% 41.3% 37.3%* 49.3%\\nAnswer in Middle Third of Context 45.3% 42.7% 37.3%* 49.0%\\nAnswer in Last Third of Context 48.0% 44.4% 37.0%* 50.9%\\n2K Context 59.1% 64.6% 36.3% 69.3%\\n4K Context 65.1% 59.9% 35.9% 63.5%\\n8K Context 59.5% 55.3% 45.0% 61.5%\\n16K Context 27.0% 20.1% 31.7% 26.0%\\n32K Context 19.9% 14.0% — 28.5%\\nTable 3: The average performance of models on the KV-Pairs and HotpotQAXL benchmarks. Bold is the highest score. Underlined is the highest score \\nother than GPT-4 Turbo. GPT-3.5 Turbo supports a maximum context length of 16K, so we could not evaluate it at 32K. *Averages for the beginning, \\nmiddle, and end of the sequence for GPT-3.5 Turbo include only contexts up to 16K.12', metadata={'source': 'genai_notes.pdf', 'page': 11}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIOne of the most popular ways to leverage a model’s context is retrieval augmented generation (RAG).  \\nIn RAG, content relevant to a prompt is retrieved from a database and presented alongside the prompt to \\ngive the model more information than it would otherwise have. Table 4 shows the quality of DBRX on two RAG \\nbenchmarks — Natural Questions and HotPotQA — when the model is also provided with the top 10 passages \\nretrieved from a corpus of Wikipedia articles using the embedding model bge-large-en-v1.5. DBRX Instruct  \\nis competitive with open models like Mixtral Instruct and LLaMA2-70B Chat and the current version  \\nof GPT-3.5 Turbo.\\nMODELDBRX  \\nINSTRUCTMIXTRAL  \\nINSTRUCTLLAMA2-70B \\nCHATGPT 3.5 TUR -\\nBO (API)GPT 4 TURBO \\n(API)\\nNatural Questions 60.0% 59.1% 56.5% 57.7% 63.9%\\nHotPotQA 55.0% 54.2% 54.7% 53.0% 62.9%\\nTable 4: The performance of the models measured when each model is given the top 10 passages retrieved from a Wikipedia corpus \\nusing bge-large-en-v1.5. Accuracy is measured by matching within the model’s answer. Bold is the highest score. Underlined is the \\nhighest score other than GPT-4 Turbo. \\nTraining Efficiency\\nModel quality must be placed in the context of how efficient the model is to train and use. This is especially \\nso at Databricks, where we build models like DBRX to establish a process for our customers to train their own \\nfoundation models.\\nWe found training mixture-of-experts models to provide substantial improvements in compute-efficiency for \\ntraining (Table 5). For example, training a smaller member of the DBRX family called DBRX MoE-B (23.5B total \\nparameters, 6.6B active parameters) required 1.7x fewer FLOPs to reach a score of 45.5% on the Databricks LLM \\nGauntlet than LLaMA2-13B required to reach 43.8%. DBRX MoE-B also contains half as many active parameters \\nas LLaMA2-13B.13', metadata={'source': 'genai_notes.pdf', 'page': 12}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AILooking holistically, our end-to-end LLM pretraining pipeline has become nearly 4x more compute-efficient \\nin the past ten months. On May 5, 2023, we released MPT-7B , a 7B parameter model trained on 1T tokens that \\nreached a Databricks LLM Gauntlet score of 30.9%. A member of the DBRX family called DBRX MoE-A (7.7B total \\nparameters, 2.2B active parameters) reached a Databricks Gauntlet score of 30.5% with 3.7x fewer FLOPs. This \\nefficiency is the result of a number of improvements, including using an MoE architecture, other architecture \\nchanges to the network, better optimization strategies, better tokenization, and - very importantly - better \\npretraining data.\\nIn isolation, better pretraining data made a substantial impact on model quality. We trained a 7B model on 1T \\ntokens (called DBRX Dense-A) using the DBRX pretraining data. It reached 39.0% on the Databricks Gauntlet \\ncompared to 30.9% for MPT-7B. We estimate that our new pretraining data is at least 2x better token-for-token \\nthan the data used to train MPT-7B. In other words, we estimate that half as many tokens are necessary to reach \\nthe same model quality. We determined this by training DBRX Dense-A on 500B tokens; it outperformed MPT-7B \\non the Databricks Gauntlet, reaching 32.1%. In addition to better data quality, another important contributor to \\nthis token-efficiency may be the GPT-4 tokenizer, which has a large vocabulary and is believed to be especially \\ntoken-efficient. These lessons about improving data quality translate directly into practices and tools that our \\ncustomers use to train foundation models on their own data.\\nMODEL TOTAL PARAMS ACTIVE PARAMS GAUNTLET SCORE RELATIVE FLOPS\\nDBRX MoE-A 7.7B 2.2B 30.5% 1x\\nMPT-7B (1T tokens) — 6.7B 30.9% 3.7x\\nDBRX Dense-A (1T tokens) — 6.7B 39.0% 3.7x\\nDBRX Dense-A (500B tokens) — 6.7B 32.1% 1.85x\\nDBRX MoE-B 23.5B 6.6B 45.5% 1x\\nLLaMA2-13B — 13.0B 43.8% 1.7x\\nTable 5:  Details of several test articles that we used to validate the training efficiency of the DBRX MoE architecture and end-to-end training pipeline.14', metadata={'source': 'genai_notes.pdf', 'page': 13}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIInference Efficiency\\nFigure 2 shows the end-to-end inference efficiency of serving DBRX and similar models using NVIDIA \\nTensorRT-LLM with our optimized serving infrastructure and 16-bit precision. We aim for this benchmark \\nto reflect real-world usage as closely as possible, including multiple users simultaneously hitting the same \\ninference server. We spawn one new user per second, each user request contains an approximately 2000 \\ntoken prompt, and each response comprises 256 tokens.\\nIn general, MoE models are faster at inference than their total parameter-counts would suggest. This is due \\nto the fact that they use relatively few parameters for each input. We find that DBRX is no exception in this \\nrespect. DBRX inference throughput is 2-3x higher than a 132B non-MoE model.\\nInference efficiency and model quality are typically in tension: bigger models typically reach higher quality, but \\nsmaller models are more efficient for inference. Using an MoE architecture makes it possible to attain better \\ntradeoffs between model quality and inference efficiency than dense models typically achieve. For example, \\nDBRX is both higher quality than LLaMA2-70B and - thanks to having about half as many active parameters - \\nDBRX inference throughput is up to 2x faster (Figure 2). Mixtral is another point on the improved pareto frontier \\nattained by MoE models: it is smaller than DBRX, and it is correspondingly lower in terms of quality but reaches \\nhigher inference throughput. Users of the Databricks Foundation Model APIs can expect to see up to 150 \\ntokens per second for DBRX on our optimized model serving platform with 8-bit quantization.15', metadata={'source': 'genai_notes.pdf', 'page': 14}),\n",
       " Document(page_content='Figure 2:  Inference throughput for various model configurations on our optimized serving infrastructure using NVIDIA TensorRT-LLM at 16-bit \\nprecision with the best optimization flags we could find. Models are run in tensor-parallel across the entire node. The input prompt contains \\napproximately 2000 prompt tokens and we generate 256 output tokens. One new user spawns every second.\\n16\\nTHE BIG BOOK OF GENERATIVE AI', metadata={'source': 'genai_notes.pdf', 'page': 15}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIHow We Built DBRX\\nDBRX was trained on 3072 NVIDIA H100s connected by 3.2Tbps Infiniband. The main process of building DBRX \\n- including pretraining, post-training, evaluation, red-teaming, and refining - took place over the course of \\nthree months. It was the continuation of months of science, dataset research, and scaling experiments, not to \\nmention years of LLM development at Databricks that includes the MPT  and Dolly  projects and the thousands \\nof models we have built and brought to production with our customers.\\nTo build DBRX, we leveraged the same suite of Databricks tools that are available to our customers. We \\nmanaged and governed our training data using Unity Catalog. We explored this data using newly acquired  \\nLilac AI . We processed and cleaned this data using Apache Spark™ and Databricks notebooks. We trained \\nDBRX using optimized versions of our open-source training libraries: MegaBlocks , LLM Foundry , Composer , \\nand Streaming . We managed large scale model training and finetuning across thousands of GPUs using our \\nMosaic AI Training service. We logged our results using MLflow . We collected human feedback for quality and \\nsafety improvements through Mosaic AI Model Serving and Inference Tables. We manually experimented with \\nthe model using the Databricks Playground. We found the Databricks tools to be best-in-class for each of their \\npurposes, and we benefited from the fact that they were all part of a unified product experience.\\nGet Started With DBRX on Databricks\\nIf you’re looking to start working with DBRX right away, it’s easy to do so with the Databricks Mosaic AI \\nFoundation Model APIs . You can quickly get started with our pay-as-you-go pricing and query the model from \\nour AI Playground  chat interface. For production applications, we offer a provisioned throughput option to \\nprovide performance guarantees, support for finetuned models, and additional security and compliance. To \\nprivately host DBRX, you can download the model from the Databricks Marketplace  and deploy the model on \\nModel Serving .17', metadata={'source': 'genai_notes.pdf', 'page': 16}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIConclusions\\nAt Databricks, we believe that every enterprise should have the ability to control its data and its destiny in the \\nemerging world of GenAI. DBRX is a central pillar of our next generation of GenAI products, and we look forward \\nto the exciting journey that awaits our customers as they leverage the capabilities of DBRX and the tools we \\nused to build it. In the past year, we have trained thousands of LLMs with our customers. DBRX is only one \\nexample of the powerful and efficient models being built at Databricks for a wide range of applications, from \\ninternal features to ambitious use-cases for our customers.\\nAs with any new model, the journey with DBRX is just the beginning, and the best work will be done by those \\nwho build on it: enterprises and the open community. This is also just the beginning of our work on DBRX, and \\nyou should expect much more to come.\\nContributions\\nThe development of DBRX was led by the Mosaic  team that previously built the MPT model family, in \\ncollaboration with dozens of engineers, lawyers, procurement and finance specialists, program managers, \\nmarketers, designers, and other contributors from across Databricks. We are grateful to our colleagues, friends, \\nfamily, and the community for their patience and support over the past months.\\nIn creating DBRX, we stand on the shoulders of giants in the open and academic community. By making DBRX \\navailable openly, we intend to invest back in the community in hopes that we will build even greater technology \\ntogether in the future. With that in mind, we gratefully acknowledge the work and collaboration of Trevor Gale  \\nand his MegaBlocks  project (Trevor’s PhD adviser is Databricks CTO Matei Zaharia), the PyTorch  team and \\nthe FSDP  project, NVIDIA  and the TensorRT-LLM  project, the vLLM  team and project, EleutherAI  and their \\nLLM evaluation  project, Daniel Smilkov and Nikhil Thorat at Lilac AI , and our friends at the Allen Institute for \\nArtificial Intelligence (AI2) .18', metadata={'source': 'genai_notes.pdf', 'page': 17}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIStage 1: Prompt Engineering\\nMany companies still remain in the foundational stages of adopting generative AI technology. They have  \\nno overarching AI strategy in place, no clear use cases to pursue and no access to a team of data scientists and \\nother professionals who can help guide the company’s AI adoption journey.\\nIf this is like your business, a good starting point is an off-the-shelf LLM. While these LLMs lack the domain-\\nspecific expertise of custom AI models, experimentation can help you plot your next steps. Your employees can \\ncraft specialized prompts and workflows  to guide their usage. Your leaders can get a better understanding of \\nthe strengths and weaknesses of these tools as well as a clearer vision of what early success in AI might look \\nlike. Your organization can use things like the Databricks AI Playground  to figure out where to invest in more \\npowerful AI tools and systems that drive more significant operational gain and even use LLMs as a judge  to help \\nevaluate responses.\\nPRACTICAL APPLICATIONS OF GENAI TECHNOLOGY\\nLet’s delve into a compelling use case that illustrates the power of prompt engineering with off-the-shelf  \\nLLMs. Consider the challenge many businesses face: sifting through vast amounts of product reviews  \\nto glean actionable insights. Without a dedicated team of data scientists or a clear AI strategy, this task  \\nmight seem daunting. However, leveraging the flexibility of LLMs through prompt engineering offers a \\nstraightforward solution.19', metadata={'source': 'genai_notes.pdf', 'page': 18}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIPrompt Engineering Use Case\\nAutomated Analysis of Product Reviews Using Large Language Models\\nKeep track of customer feedback at scale\\nCheck out our  LLM Solution Accelerators for Retail  for more details and to download the notebooks.\\nWhile conversational AI has garnered a lot of media attention in recent months, the capabilities of large  \\nlanguage models (LLMs) extend well beyond conversational interactions. It's in these less prominent  \\ncapabilities such as query response, summarization, classification and search that many organizations  \\nare finding immediate opportunities to supercharge their workforce and up-level customer experiences.\\nThe potential of these applications is staggering. By one estimate , LLMs (and other generative AI  \\n technologies) could, in the near future, address tasks that today occupy 60%–70% of employees’ time.  \\nThrough augmentation, numerous studies  have shown that the time to complete various tasks performed  \\nby knowledge workers such as background research, data analysis and document writing can be cut in half.  \\nAnd still other studies  have shown that the use of these technologies can dramatically reduce the time for  \\nnew workers to achieve full productivity.\\nBut before these benefits can be fully realized, organizations must first rethink  the management of the \\nunstructured information assets on which these models depend and find ways to mitigate the issues of bias \\nand accuracy that affect their output. This is why so many organizations are currently focusing their efforts \\non focused, internal applications where a limited scope provides opportunities for better information access \\nand human oversight can serve as a check to errant results. These applications, aligned with core capabilities \\nalready residing within the organization, have the potential to deliver real and immediate value, while LLMs and \\ntheir supporting technologies continue to evolve and mature.20\", metadata={'source': 'genai_notes.pdf', 'page': 19}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIPRODUCT REVIEW SUMMARIZATION COULD USE A BOOST\\nTo illustrate the potential of a more focused approach to LLM adoption, we consider a fairly simple and common \\ntask performed within many online retail organizations: product review summarization. Today, most organizations \\nemploy a modestly-sized team of workers to read and digest user feedback for insights that may help improve a \\nproduct's performance or otherwise identify issues related to customer satisfaction.\\nThe work is important but anything but sexy. A worker reads a review, takes notes, and moves on to the next. \\nIndividual reviews that require a response are flagged and a summary of the feedback from across multiple \\nreviews are compiled for review by product or category managers.\\nThis is a type of work that's ripe for automation. The volume of reviews that pour into a site mean the more \\ndetailed portions of this work are often performed on a limited subset of products across variable windows \\ndepending on a products importance. In more sophisticated organizations, rules detecting course or \\ninappropriate language and models estimating user sentiment or otherwise classifying reviews for positive, \\nnegative or neutral experiences may be applied to help identify problematic content and draw a reviewer's \\nattention to it. But either way, a lot is missed simply because we can't throw enough bodies at the problem to \\nkeep up and those bodies tend to become bored or fatigued with the monotony of the work.\\nLARGE LANGUAGE MODELS CAN AUTOMATE PRODUCT REVIEW ANALYSIS\\nBy using an LLM, issues of scale and consistency can be easily addressed. All we need to do is bring the product \\nreviews to the model and ask:\\n ■What are the top three points of negative feedback found across these reviews?\\n ■What features do our customers like best about this product?\\n ■Do customers feel they are receiving sufficient value from the product relative to what they are being \\nasked to pay?\\n ■Are there any reviews that are especially negative or are using inappropriate language?21\", metadata={'source': 'genai_notes.pdf', 'page': 20}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIWithin seconds we can have a tidy response, allowing our product managers to focus on responding to issues \\ninstead of simply detecting them.\\nBut what about the problem of accuracy and bias? Standards for identifying inaccuracies and bias in LLM \\noutput are evolving as are techniques for better ensuring that outputs align with an organization's expectations, \\nand the fine-tuning of models using approved content can go a long way to ensure models have a preference to \\ngenerate content that's at least aligned with how an organization prefers to communicate.\\nThis is a long-winded way of saying there is no ideal solution to the problem as of yet. But when compared  \\nto where we are with human-driven processes and more simplistic models or rules-based approaches,  \\nthe results are expected to be better or at a minimum no worse than what we currently experience.  \\nAnd given that these review summaries are for internal consumption, the impact of an errant model can  \\nbe easily managed.\\nYOU CAN BUILD A SOLUTION FOR THIS TODAY\\nTo demonstrate exactly how this work could be performed, we have built a Solution Accelerator  for summarizing \\nproduct reviews. This is based heavily on a previously published blog  from Sean Owen that addressed some of \\nthe core technical challenges of tuning an LLM on the Databricks platform. For the accelerator, we are using the \\nAmazon Product Reviews Dataset , which contains 51 million user-generated reviews across 2 million distinct \\nbooks as this provides access to a wide range of reviewer content and presents a scaling challenge many \\norganizations will recognize.\\nWe imagine a scenario in which a team of product managers receives customer feedback through online \\nreviews. These reviews are important for identifying issues that may need to be addressed regarding a \\nparticular item and for steering future books to be offered by the site. Without the use of technology, this team \\nstruggles to read all the feedback and summarize into a workable set notes. As a result, they limit their attention \\nto just the most critical items and are able to only process the feedback on a sporadic basis.22\", metadata={'source': 'genai_notes.pdf', 'page': 21}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIBut using Databricks, they are able to set up a pipeline to collect feedback from a wider range of products \\nand summarize these on a regular basis. Recognizing that positively rated products are likely to highlight the \\nstrengths of these books while lower rated products are likely to focus on their weaknesses, they separate  \\nthese reviews based on user-provided ratings and task an LLM to extract different sets of information from  \\neach high-level category of reviews.\\nSummary metrics are provided to allow product managers an overview of the feedback received and are \\nbacked by more detailed summaries generated by the LLM (Figure 1).\\nFigure 1: Summary metrics and bullet-point details extracted from user reviews extracted using an LLM\\n23', metadata={'source': 'genai_notes.pdf', 'page': 22}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIDATABRICKS BRINGS TOGETHER ALL THE COMPONENTS OF A SOLUTION\\nThe scenario demonstrated above depends on the use of an LLM. In months prior, the use of such an LLM \\nrequired access to specialized computational infrastructures, but with advances in the open source community \\nand investments in the Databricks platform, we are now able to run the LLM in our local Databricks environment.\\nIn this particular scenario, the sensitivity of the data was not a motivating factor for this choice. Instead, we \\nfound that the volume of reviews to be processed tipped the cost scales toward the use of Databricks, allowing \\nus to trim about one-third of the cost of implementing a similar solution using a third-party service.\\nIn addition, we found that by implementing our own infrastructure, we were able to scale the environment up \\nfor faster processing, tackling as many as 760,000 reviews per hour in one test without having to be concerned \\nwith constraints imposed by an external service. While most organizations will not have the need to scale quite \\nto that level, it's nice to know it is there should it be.\\nBut this solution is more than just an LLM. To bring together the whole solution we needed to develop a data \\nprocessing workflow to receive incoming reviews, prepare them for submission to the model and to capture \\nmodel output for further analysis. As a unified data platform, Databricks provides us the means to address  \\n both data engineering and data science requirements without data replication. And when we are done \\nprocessing the reviews, our analysts can use their tools of choice to query the output and make business \\ndecisions. Through Databricks, we have access to the full array of capabilities for us to build a solution aligned \\nwith our business’ needs.24\", metadata={'source': 'genai_notes.pdf', 'page': 23}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIStage 2: Retrieval Augmented Generation\\nRetrieval augmented generation (RAG) lets you bring in supplemental knowledge resources to make an  \\noff-the-shelf AI system smarter. RAG won’t change the underlying behavior of the model, but it will improve  \\nthe quality  and accuracy of the responses.\\nHowever, at this point, your business should not be uploading its “mission-critical” data. Instead, the RAG \\nprocess typically involves smaller amounts of nonsensitive information.\\nFor example, plugging in an employee handbook can allow your workers to start asking the underlying model \\nquestions about the organization’s vacation policy. Uploading instruction manuals can help power a service \\nchatbot. With the ability to query support tickets using AI, support agents can get answers quicker; however, \\ninputting confidential financial data so employees can inquire about the company’s performance is likely a step \\ntoo far.\\nTo get started, your team should first consolidate and cleanse the data you intend to use. With RAG, it’s vital \\nthat your company stores the data in sizes that will be appropriate for the downstream models. Often, that \\nrequires users to splice it into smaller segments.\\nThen, you should seek out a tool like Databricks Vector Search , which enables users to quickly set up their own \\nvector database. And because it’s governed by Unity Catalog, granular controls can be put in place to ensure \\nemployees are only accessing the datasets for which they have credentials.\\nFinally, you can then plug that endpoint into a LLM. A tool like Databricks MLflow helps to centralize the \\nmanagement of those APIs.25', metadata={'source': 'genai_notes.pdf', 'page': 24}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIAmong the benefits of RAG  are reduced hallucinations, more up-to-date and accurate responses,  \\nand better domain-specific intelligence. RAG-assisted models are also a more cost-effective approach  \\nfor most organizations.\\nWhile RAG will help improve the results from commercial models, there are still many limitations to the use \\nof RAG. If your business is unable to get the results it wants, it’s time to move on to heavier-weight solutions, \\nbut moving beyond RAG-supported models often requires a much deeper commitment. The additional \\ncustomization costs more and requires a lot more data.\\nThat’s why it’s key that organizations first build a core understanding of how to use LLMs. By reaching the \\nperformance limitations of off-the-shelf models before moving on, you and your leadership can further hone  \\nin on where to allocate resources.\\nEnhance the Performance of Off-the-Shelf AI Models With RAG\\nLet’s explore a practical use case that demonstrates how real-time structured data can significantly improve \\nthe response quality of your RAG applications. This example will showcase how integrating dynamic information \\ncan transform the effectiveness and applicability of AI in your business operations.\\n26', metadata={'source': 'genai_notes.pdf', 'page': 25}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIRAG Use Case\\nImprove Your RAG Application Response Quality With Real-Time Structured Data\\nby Mani Parkhe , Aakrati Talati , Sue Ann Hong , Craig Wiley , Chenen Liang  and Mingyang Ge\\nRetrieval augmented generation (RAG)  is an efficient mechanism to provide relevant data as context in \\nGenAI applications. Most RAG applications typically use vector indexes to search for relevant context from \\nunstructured data such as documentation, wikis, and support tickets. Yesterday, we announced Databricks \\nVector Search Public Preview that helps with exactly that. However, GenAI response quality can be enhanced by \\naugmenting these text-based contexts with relevant and personalized structured data. Imagine a GenAI tool on \\na retail website where customers inquire, \"Where’s my recent order?\" This AI must understand that the query \\nis about a specific purchase, then gather up-to-date shipment information for line items, before using LLMs to \\ngenerate a response. Developing these scalable applications demands substantial work, integrating technologies \\nfor handling both structured and unstructured data with GenAI capabilities.\\nWe are excited to announce the public preview of Databricks Feature & Function Serving, a low latency real-\\ntime service designed to serve structured data from the Databricks Data Intelligence Platform. You can instantly \\naccess pre-computed ML features as well as perform real-time data transformations by serving any Python \\nfunction from Unity Catalog. The retrieved data can then be used in real-time rule engines, classical ML, and \\nGenAI applications.\\nUsing Feature and Function Serving ( AWS )(Azure) for structured data in coordination with Databricks Vector \\nSearch ( AWS )(Azure) for unstructured data significantly simplifies productionalization of GenAI applications. \\nUsers can build and deploy these applications directly in Databricks and rely on existing data pipelines, \\ngovernance, and other enterprise features. Databricks customers across various industries are using these \\ntechnologies along with open source frameworks to build powerful GenAI applications such as the ones \\ndescribed in the table below.27', metadata={'source': 'genai_notes.pdf', 'page': 26}),\n",
       " Document(page_content='INDUSTRY USE CASE\\nRetail  ■Product Recommendations / Search Ranking using user preferences, search history, location . . . etc.\\n ■Image and metadata based product search\\n ■Inventory management and forecasting using sales data, seasonal trends and market/competitive analysis\\nEducation  ■Personalized learning plans based on past mistakes, historical trends and  cohorts\\n ■Automated grading, feedback, follow-ups and progress reporting\\n ■Content filtering for issued devices\\nFinancial Services  ■Natural language apps for analysts and investors to correlate earning calls and reports with market intelligence and historical trends\\n ■Fraud and risk analysis\\n ■Personalized wealth management, retirement planning, what-if analysis and next-best actions \\nTravel and Hospitality  ■Chatbots for personalized customer interactions and tailored travel recommendations\\n ■Dynamic route planning using weather, live traffic patterns, and historical data\\n ■Dynamic price optimization using competitive analysis and demand-based pricing\\nHealthcare and Life Sciences  ■Patient/member engagement and health summaries\\n ■Support apps for personalized care, clinical decisions and care coordination\\n ■R&D report summarization, clinical trial analysis, drug repurposing\\nInsurance  ■Risk assessment for mortgage underwriting using text and structured data about properties and neighborhoods\\n ■User chatbots for questions about policies, risk and what-if analysis\\n ■Claim processing automation\\nTechnology and Manufacturing  ■Prescriptive maintenance and diagnostics for equipment using guided instruction\\n ■Anomaly detection on live data stream against historical statistics\\n ■Automated analysis for daily production / shift analysis and future planning\\nMedia and Entertainment  ■In-app content discovery and recommendations, personalized email and digital marketing\\n ■Content localization\\n ■Personalized gaming experiences and game review28\\nTHE BIG BOOK OF GENERATIVE AI', metadata={'source': 'genai_notes.pdf', 'page': 27}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AISERVING STRUCTURED DATA TO RAG APPLICATIONS\\nTo demonstrate how structured data can help enhance the quality of a GenAI application, we use the following \\nexample for a travel planning chatbot. The example shows how user preferences (example: “ocean view” or \\n“family friendly”) can be paired with unstructured information sourced about hotels to search for hotel matches. \\nTypically hotel prices dynamically change based on demand and seasonality. A price calculator built into the \\nGenAI application ensures that the recommendations are within the user's budget. The GenAI application that \\npowers the bot uses Databricks Vector Search and Databricks Feature and Function Serving as building blocks \\nto serve the necessary personalized user preferences and budget and hotel information using LangChain’s \\nagents API.\\nYou can find the complete notebook  for this RAG Chain application as depicted above. This application can be \\nrun locally within the notebook or deployed as an endpoint accessible by a chatbot user interface.29\", metadata={'source': 'genai_notes.pdf', 'page': 28}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIACCESS YOUR DATA AND FUNCTIONS AS REAL-TIME ENDPOINTS\\nWith Feature Engineering in Unity Catalog you can already use any table with a primary key to serve features \\nfor training and serving. Databricks Model Serving supports using Python functions to compute features on-\\ndemand . Built using the same technology available under the hood for Databricks Model Serving, feature and \\nfunction endpoints can be used to access any pre-computed feature or compute them on-demand. With a \\nsimple syntax you can define a feature spec function in Unity Catalog that can encode the directed acyclic \\ngraph to compute and serve features as a REST endpoint.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28from databricks.feature_engineering import (\\n  FeatureFunction,\\n  FeatureLookup,\\n  FeatureEngineeringClient,\\n)\\nfeatures = [\\n  # Lookup columns `latitude` and `longitude` from `restaurants` table in UC using the input `restaurant_id` as key\\n  FeatureLookup(\\n    table_name=\"main.default.restaurants\",\\n    lookup_key=\"restaurant_id\",\\n    features=[\"latitude”, “longitude\"]\\n  ),\\n  # Calculate a new feature called `distance` using the restaurant and user\\'s current location\\n  FeatureFunction(\\n    udf_name=\"main.default.distance\",\\n    output_name=\"distance\",\\n    # bind the function parameter with input from other features or from request.\\n    input_bindings={\"user_latitude\": \"user_latitude\", \"user_longitude\": \"user_longitude\",\\n                    \"restaurant_latitude\": \"latitude\", \"restaurant_longitude\": \"longitude\"},\\n  ),\\n]\\nfe = FeatureEngineeringClient()\\n# Create a feature spec with the features listed above.\\n# The FeatureSpec can be accessed in UC as a Function.\\nfe.create_feature_spec(\\n  name=\"main.default.restaurant_features\",\\n  features=features,\\n)30', metadata={'source': 'genai_notes.pdf', 'page': 29}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4\\n \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15from databricks.feature_engineering.entities.feature_serving_endpoint import (\\n  ServedEntity,\\n  EndpointCoreConfig,\\n)\\nfe.create_feature_serving_endpoint(\\n  name=\"restaurant-features\",\\n    config=EndpointCoreConfig(\\n    served_entities=ServedEntity(\\n      feature_spec_name=\"main.default.restaurant_features\",\\n      workload_size=\"Small\",\\n      scale_to_zero_enabled=True\\n    )\\n  )\\n)This feature spec function can be served in real-time as a REST endpoint. All endpoints are accessible in \\nthe Serving left navigation tab including features, function, custom trained models, and foundation models. \\nProvision the endpoint using this API.\\nThe endpoint can also be created using a UI workflow as shown in the following graphic\\n31', metadata={'source': 'genai_notes.pdf', 'page': 30}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AINow features can be accessed in real time by querying the endpoint:\\nTo serve structured data to real-time AI applications, precomputed data needs to be deployed to operational \\ndatabases. Users can already use external online stores as a source of precomputed features — for example \\nDynamoDB  and Cosmos DB  are commonly used to serve features in Databricks Model Serving. Databricks \\nOnline Tables ( AWS )(Azure) adds new functionality that simplifies synchronization of precomputed features to a \\ndata format optimized for low latency data lookups. You can sync any table with a primary key as an online table \\nand the system will set up an automatic pipeline to ensure data freshness. \\n1\\n2\\n3\\n4\\n5\\n \\n6curl \\\\\\n  -u token:$DATABRICKS_TOKEN \\\\\\n  -X POST \\\\\\n  -H \"Content-Type: application/json\" \\\\\\n  -d \\'{\"dataframe_records\": [{\"user_latitude\": 37.9711, \"user_longitude\": -122.3940, \"restaurant_id\": 5}]}\\' \\\\  \\n  https://<databricks-instance>/serving-endpoints/restaurant-features/invocations\\n32', metadata={'source': 'genai_notes.pdf', 'page': 31}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIStage 3: Fine-Tuning a Foundation Model\\nMoving beyond RAG to model fine-tuning lets you start building models that are much more deeply \\npersonalized to the business. If you have already been experimenting with commercial models across your \\noperations, you are likely ready to advance to this stage. There’s a clear understanding at the executive level of \\nthe value of generative AI, as well as an understanding of the limitations of publicly available LLMs. Specific use \\ncases have been established. And now, you and your enterprise are ready to go deeper.\\nWith fine-tuning, you can take a general-purpose model and train it on your own specific data. For example, \\ndata management provider Stardog relies on the Mosaic AI tools from Databricks  to fine-tune the off-the-shelf \\nLLMs they use as a foundation for their Knowledge Graph Platform. This enables Stardog’s customers to query \\ntheir own data across the different silos simply by using natural language.\\nIt’s imperative that organizations at this stage have an underlying architecture in place that will help ensure the \\ndata supporting the models is secure and accurate. Fine-tuning an AI system requires an immense amount of \\nproprietary information and, as your business advances on the AI maturity curve, the number of models running \\nwill only grow, increasing the demand for data access.\\nThat’s why you need to have the right mechanisms in place to track data from the moment it’s generated to \\nwhen it’s eventually used, and why Unity Catalog  is such a popular feature among Databricks customers. With \\nUnity Catalog’s data lineage capabilities, businesses always know where data is moving and who is accessing it.Any Unity Catalog table with primary keys can be used to serve features in GenAI applications using Databricks \\nOnline Tables.33', metadata={'source': 'genai_notes.pdf', 'page': 32}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIFine-Tuning Use Cases\\nCreating a Bespoke LLM for AI-Generated Documentation  \\nIt’s easier than you think: 2 engineers, 1 month and less than $1,000\\nby Matthew Hayes , Hongyi Zhang , Tao Feng , Jan van der Vegt , Zaheera Valani  and Reynold Xin\\nIn this example, we share our experience from prototyping a hackathon project using off-the-shelf SaaS-based \\nLLMs to creating a bespoke LLM that is better, faster, and cheaper. The new model took 2 engineers, 1 month \\nand less than $1,000 in compute cost to develop. We hope you will find the learnings useful, as we believe \\nthey apply to a wide class of GenAI use cases. More importantly, it has allowed us to take advantage of rapid \\nadvances being made in open-source LLMs.\\nWHAT IS AI-GENERATED DOCUMENTATION?\\nAt the center of each data platform lies a (potentially enormous) collection of datasets (often in the form of \\ntables). In virtually every organization we have worked with, the vast majority of tables are not documented. The \\nabsence of documentation provides a number of challenges, including making it difficult for humans to discover \\nthe data needed for answering a business question, or more recently, for AI agents to automatically find \\ndatasets to use in response to questions (a key capability in our platform that we’re calling Data Intelligence ).34', metadata={'source': 'genai_notes.pdf', 'page': 33}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIRather than relying on humans to document these datasets, we prototyped as part of our quarterly hackathon \\na new workflow using an off-the-shelf SaaS-based LLM to automatically generate documentation for tables \\nand their columns based on their schema. This new workflow would automatically suggest descriptions for the \\ntables and columns and allow users to either individually accept, bulk accept, or modify the suggestions for \\nhigher fidelity, as shown below. When we showed this prototype to some users, their immediate question was \\nuniversally, “When can I have it?!”\\n35', metadata={'source': 'genai_notes.pdf', 'page': 34}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AICHALLENGES WITH LLMS\\nAs we moved toward launching this feature to all our customers, we ran into three challenges with the model:\\n1. Quality:  The ultimate success of this feature depends on the quality of the generated documentation. \\nAlthough we could measure the quality (in terms of how often they are accepted), we had limited knobs \\nat our disposal to improve it, aside from basic prompting. During the private preview period, we also \\nsometimes noticed the quality of the suggestions degrading, without any change to our codebase. Our \\nspeculation is that the SaaS LLM controller rolled out updates to the model that sometimes affected \\nperformance on specific tasks.\\n2. Performance (throughput):  We had limited API quota provisioned with the SaaS LLM provider. We \\nwork with tens of thousands of organizations, and it is not uncommon that a single organization would \\nhave millions of tables. It would take too long to generate documentation for all the tables based on the \\nthroughput quota.\\n3. Cost:  Related to the above, it was not cost-effective unless we started charging customers for using this \\nspecific feature.\\nWe have heard similar concerns from a variety of customers as they try to move their LLM-based applications \\nfrom a proof-of-concept to production and saw this as an excellent opportunity for us to explore alternatives \\nfor an organization like ours.\\nWe experimented with different versions of the SaaS LLMs, but they all had the same challenges. This is not \\nsurprising in hindsight. The SaaS LLMs are an engineering marvel, but they are very general models that need to \\naddress all the use cases from table generation to conversing about the meaning of life. The generality means \\nit needs to have an extremely large number of parameters, which limits how fast and how cheap it can return \\nanswers. As it continues to evolve to optimize for different use cases, it might also regress the narrower use case \\nwe have.36', metadata={'source': 'genai_notes.pdf', 'page': 35}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIBUILDING A BESPOKE MODEL\\nTo address the aforementioned challenges, we started building a bespoke model. It took a team of two \\nengineers one month to build a customized, smaller LLM that was better, faster, and cheaper:\\n ■Quality: Based on our evaluation (see the following section), the model is significantly better than the \\ncheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■Performance (throughput):  Because the bespoke model is a lot smaller, it can fit in A10 GPUs, and we \\ncan increase the inference throughput with horizontal scaling. The smaller GPUs are also more available, \\nwhich enables us to generate the descriptions for all tables faster.\\n ■Cost: Each fine-tuning run of the model only costs a few dollars, and in aggregate, it cost less than $1000 \\nto develop because we did a lot of experiments. It also resulted in a 10 fold reduction in inference cost.\\nThe first step was to treat this as an applied machine learning problem. “Applied machine learning” sounds \\ndaunting and complicated, but all it meant was that we needed to:\\n ■Find training datasets so we can bootstrap an initial model\\n ■Identify an evaluation mechanism so we can measure the quality, before rolling it out to production\\n ■Train and select models\\n ■Collect real-world usage metrics, so we can monitor how well a monitor does in production\\n ■Iterate and roll out new models to continuously improve the three dimensions: quality, performance, cost37', metadata={'source': 'genai_notes.pdf', 'page': 36}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AITRAINING DATA\\nWe created the initial training dataset for this fine-tuning task, using two different sources of data:\\n1. North American Industry Classification System (NAICS) codes. This is a public dataset used by Federal \\nstatistical agencies in classifying business establishments for the purpose of collecting, analyzing, and \\npublishing statistical data related to the U.S. business economy.\\n2. Databricks’ internal use case taxonomy curation datasets. This is a series of internal datasets created by \\nour solution architects to show customers best practice architectures.\\nThen we synthesized CREATE TABLE statements using the above use cases to yield a diverse set of tables and \\ngenerated sample responses including table descriptions and column comments using another LLM. In total,  \\nwe generated ~3600 training examples. \\nNotably, we didn’t use any customer data for training this powerful feature that all of our customers can  \\nbenefit from. \\nBOOTSTRAPPING MODEL EVALUATION\\nAfter the feature launch, we could measure a model’s quality through production metrics such as the rate of \\nusers accepting the suggestions. But before we made it to the launch, we needed a way to evaluate the model’s \\nquality against that of the SaaS LLM.\\nTo do that in an unbiased fashion, we set up a simple double-blind evaluation framework in which we asked \\n4 employees to rate table descriptions generated from the two models we wanted to compare using a set of \\n62 unseen tables. Our framework then generated a sheet where each row showed the input and showed both \\noutputs in a randomized order. The evaluator would vote on the better sample (or give a tie). The framework then \\nprocessed the votes from different evaluators to generate a report; it also summarizes the degree to which each \\nof the evaluators agreed.\\nBased on our experiences so far, having an evaluation dataset of tens to hundreds of data points is a sufficient \\ninitial milestone and can be generalized to other use cases as well.38', metadata={'source': 'genai_notes.pdf', 'page': 37}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIMODEL SELECTION AND FINE-TUNING\\nWe considered the following criteria for model selection:\\n ■Whether the license supports commercial use\\n ■Performance (quality) of the model for text generation\\n ■Speed of the model\\nBased on these criteria, MPT-7B and Llama2-7B were the leading candidates, as shown in our LLM guide . We \\nconsidered larger models such as MPT-30B and Llama-2-13B. In the end we chose MPT-7B, as it has the best \\ncombination of quality and inference performance:\\n ■There was no discernable difference in the quality between the MPT-7B and Llama-2-7B fine-tuned \\nmodels for this task.\\n ■The smaller 7B models, after fine-tuning, were already meeting the quality bar. It was significantly better \\nthan the cheaper version of the SaaS model, and roughly equivalent to the more expensive version.\\n ■We did not yet observe a measurable benefit of using larger models for this task that would justify the \\nincreased serving costs.\\n ■The latency for the smaller models was significantly better than the larger models while offering \\ncomparable quality so we could deliver a much snappier product experience.\\n ■The smaller model could fit comfortably and be served using A10 GPUs, which were more readily \\navailable. Their abundance would mean higher inference throughput for the task.\\nThe total time it took to fine-tune the model on the ~3600 examples was only around 15 minutes!\\nWhile we chose MPT-7B for our model, we believe the LLM landscape is changing rapidly and the best model \\ntoday won’t be the best model tomorrow. That’s why we consider this to be an iterative and continuous process \\nand are focused on using tools that make our evaluation efficient and fast.39', metadata={'source': 'genai_notes.pdf', 'page': 38}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIKEY ARCHITECTURAL COMPONENTS OF OUR PRODUCTION PIPELINE\\nWe were able to build this quickly by relying on the following key components of the Databricks Data \\nIntelligence Platform:\\n ■Databricks LLM fine-tuning:  It provides a very simple infrastructure for fine-tuning the models for our \\ntask. We prepared the training data in JSON format, and with a one-line CLI command, we were able to \\nfine-tune the LLMs.\\n ■Unity Catalog: The models that we use in production are registered in Unity Catalog (UC), providing the \\ngovernance we need to not just for the data, but also the models. With its end-to-end lineage feature, UC \\nalso gives us traceability from the models back to the datasets they are trained on.\\n ■Delta Sharing:  We used Delta Sharing to distribute the model to all production regions we have around \\nthe world for faster serving.\\n ■Databricks optimized LLM serving : Once the models are registered in UC, they can be served using the \\nnew optimized LLM serving, which provides significant performance improvement in terms of throughput \\nand latency improvement compared to traditional serving for LLM serving.40', metadata={'source': 'genai_notes.pdf', 'page': 39}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AICOST\\nThe fine-tuning compute cost for the whole project was less than $1000 (each fine-tuning run cost only a few \\ndollars). And the final result is a more than 10-fold reduction in cost. Why is the cost-saving so significant? It is \\nnot surprising if we consider the following:\\n ■As mentioned earlier, the SaaS LLMs need to address all the use cases, including acting as a general \\nchatbot. The generality requires an extremely large number of parameters, which incurs significant \\ncompute costs in inference.\\n ■When we fine-tune for a more specific task, we can use a much smaller prompt. Larger, general-purpose \\nmodels require longer prompts that include detailed instructions on what the input is and what form the \\noutput should take. Fine-tuned models can bake instructions and expected structure into the model \\nitself. We found we were able to reduce the number of input tokens with no impact on performance by \\nmore than half.\\n ■Inference costs scale with the number of input and output tokens, and costs scale linearly for SaaS \\nservices that are charged per token. With Databricks’ LLM Serving offering, we offer provisioned \\nthroughput charged per hour, which provides consistent latencies, uptime SLAs, and autoscaling. Because \\nsmaller LLMs can fit in smaller GPUs that are much cheaper and more available and because we offer a \\nhighly optimized runtime, we can aggressively drive down costs. Also, smaller LLMs scale up and down \\nfaster, meaning we can quickly scale up to meet peaks of demand and aggressively scale down when \\nusage is lighter, creating substantial cost efficiency in production.41', metadata={'source': 'genai_notes.pdf', 'page': 40}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AICONCLUSION\\nHaving well-documented data is critical to all data users, and growing more important day-by-day to power \\nAI-based data platforms (what we’re calling Data Intelligence ). We started with SaaS LLMs for prototyping this \\nnew GenAI feature but ran into challenges with quality, performance, and cost. We built a bespoke model to do \\nthe same task at better quality, and yet resulting in higher throughput with scale-out and 10x cost reduction. To \\nrecap what it took:\\n ■2 engineers\\n ■1 month\\n ■Less than $1,000 in compute for training and experimentation\\n ■MPT-7B fine-tuned on 3600 synthetically generated examples, in under 15 minutes\\n ■4 human evaluators, with 62 initial evaluation examples\\nThis experience demonstrates how easy it is to develop and deploy bespoke LLMs for specific tasks. This model \\nis now live on Databricks in Amazon Web Services and Google Cloud and is being used to power most data \\nannotations on the platform.42', metadata={'source': 'genai_notes.pdf', 'page': 41}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIEfficient Fine-Tuning With LoRA: A Guide to Optimal Parameter Selection for Large Language Models\\nby Avinash Sooriyarachchi\\nWith the rapid advancement of neural network-based techniques and large language model (LLM) research, \\nbusinesses are increasingly interested in AI applications for value generation. They employ various machine \\nlearning approaches, both generative and non-generative, to address text-related challenges such as \\nclassification, summarization, sequence-to-sequence tasks, and controlled text generation. Organizations can \\nopt for third-party APIs, but fine-tuning models with proprietary data offers domain-specific and pertinent \\nresults, enabling cost-effective and independent solutions deployable across different environments in  \\na secure manner.\\nEnsuring efficient resource utilization and cost-effectiveness is crucial when choosing a strategy for fine-\\ntuning. This blog explores arguably the most popular and effective variant of such parameter efficient \\nmethods, Low Rank Adaptation (LoRA), with a particular emphasis on QLoRA (an even more efficient variant of \\nLoRA). The approach here will be to take an open large language model and fine-tune it to generate fictitious \\nproduct descriptions when prompted with a product name and a category. The model chosen for this \\nexercise is OpenLLaMA-3b-v2 , an open large language model with a permissive license (Apache 2.0), and the \\ndataset chosen is Red Dot Design Award Product Descriptions , both of which can be downloaded from the \\nHuggingFace Hub at the links provided.\\nFINE-TUNING, LORA AND QLORA\\nIn the realm of language models, fine-tuning an existing language model to perform a specific task on specific \\ndata is a common practice. This involves adding a task-specific head, if necessary, and updating the weights of \\nthe neural network through backpropagation during the training process. It is important to note the distinction \\nbetween this fine-tuning process and training from scratch. In the latter scenario, the model's weights are \\nrandomly initialized, while in fine-tuning, the weights are already optimized to a certain extent during the \\npretraining phase. The decision of which weights to optimize or update, and which ones to keep frozen, depends \\non the chosen technique.\\nFull fine-tuning involves optimizing or training all layers of the neural network. While this approach typically \\nyields the best results, it is also the most resource-intensive and time-consuming.43\", metadata={'source': 'genai_notes.pdf', 'page': 42}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIFortunately, there exist parameter-efficient approaches for fine-tuning that have proven to be effective. \\nAlthough most such approaches have yielded less performance, Low Rank Adaptation (LoRA) has bucked \\nthis trend by even outperforming full fine-tuning in some cases, as a consequence of avoiding catastrophic \\nforgetting (a phenomenon which occurs when the knowledge of the pretrained model is lost during the  \\nfine-tuning process).\\nLoRA is an improved fine-tuning method where instead of fine-tuning all the weights that constitute the  \\nweight matrix of the pretrained large language model, two smaller matrices that approximate this larger matrix \\nare fine-tuned. These matrices constitute the LoRA adapter. This fine-tuned adapter is then loaded to the \\npretrained model and used for inference.\\nQLoRA is an even more memory efficient version of LoRA where the pretrained model is loaded to GPU  \\nmemory as quantized 4-bit weights (compared to 8-bits in the case of LoRA), while preserving similar \\neffectiveness to LoRA. Probing this method, comparing the two methods when necessary, and figuring out the \\nbest combination of QLoRA hyperparameters to achieve optimal performance with the quickest training time \\nwill be the focus here.\\nLoRA is implemented in the Hugging Face Parameter Efficient Fine-Tuning (PEFT) library, offering ease  \\nof use and QLoRA can be leveraged by using bitsandbytes  and PEFT together. HuggingFace Transformer \\nReinforcement Learning (TRL)  library offers a convenient trainer for supervised fine-tuning with seamless \\nintegration for LoRA. These three libraries will provide the necessary tools to fine-tune the chosen pretrained \\nmodel to generate coherent and convincing product descriptions once prompted with an instruction  \\nindicating the desired attributes.44', metadata={'source': 'genai_notes.pdf', 'page': 43}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIPREPPING THE DATA FOR SUPERVISED FINE-TUNING\\nTo probe the effectiveness of QLoRA for fine-tuning a model for instruction following, it is essential to transform \\nthe data to a format suited for supervised fine-tuning. Supervised fine-tuning in essence, further trains a \\npretrained model to generate text conditioned on a provided prompt. It is supervised in that the model is fine-\\ntuned on a dataset that has prompt-response pairs formatted in a consistent manner.\\nAn example observation from our chosen dataset from the Hugging Face hub looks as follows:\\nAs useful as this dataset is, this is not well formatted for fine-tuning of a language model for instruction following \\nin the manner described.\\nThe following code snippet loads the dataset from the Hugging Face hub into memory, transforms the \\nnecessary fields into a consistently formatted string representing the prompt, and inserts the response (i.e., \\nthe description), immediately afterward. This format is known as the ‘Alpaca format’ in large language model \\nresearch circles as it was the format used to fine-tune the original LlaMA model from Meta to result in the \\nAlpaca model, one of the first widely distributed instruction-following large language models (although not \\nlicensed for commercial use).PRODUCT CATEGORY DESCRIPTION TEXT\\n“Biamp Rack Products” “Digital Audio Processors\" “High recognition value, uniform \\naesthetics and practical \\nscalability — this has been \\nimpressively achieved with the \\nBiamp brand language . . . ““Product Name: Biamp Rack Products; \\nProduct Category: Digital Audio \\nProcessors; Product Description: High \\nrecognition value, uniform aesthetics \\nand practical scalability — this has been \\nimpressively achieved with the Biamp \\nbrand language . . . “45', metadata={'source': 'genai_notes.pdf', 'page': 44}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26import pandas as pd\\nfrom datasets import load_dataset\\nfrom datasets import Dataset\\n#Load the dataset from the HuggingFace Hub\\nrd_ds = load_dataset(\"xiyuez/red-dot-design-award-product-description\")\\n#Convert to pandas dataframe for convenient processing\\nrd_df = pd.DataFrame(rd_ds[\\'train\\'])\\n#Combine the two attributes into an instruction string\\nrd_df[\\'instruction\\'] = \\'Create a detailed description for the following product: \\'+ rd_df[\\'product\\']+\\', belonging to \\ncategory: \\'+ rd_df[\\'category\\']\\nrd_df = rd_df[[\\'instruction\\', \\'description\\']]\\n#Get a 5000 sample subset for fine-tuning purposes\\nrd_df_sample = rd_df.sample(n=5000, random_state=42)\\n#Define template and format data into the template for supervised fine-tuning\\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the \\nrequest.\\n### Instruction:\\n{}\\n### Response:\\\\n\"\"\"\\nrd_df_sample[\\'prompt\\'] = rd_df_sample[\"instruction\"].apply(lambda x: template.format(x))\\nrd_df_sample.rename(columns={\\'description\\': \\'response\\'}, inplace=True)\\nrd_df_sample[\\'response\\'] = rd_df_sample[\\'response\\'] + \"\\\\n### End\"\\nrd_df_sample = rd_df_sample[[\\'prompt\\', \\'response\\']]\\nrd_df[\\'text\\'] = rd_df[\"prompt\"] + rd_df[\"response\"]\\nrd_df.drop(columns=[\\'prompt\\', \\'response\\'], inplace=True)\\nThe resulting prompts are then loaded into a hugging face dataset for supervised fine-tuning. Each such prompt \\nhas the following format.46', metadata={'source': 'genai_notes.pdf', 'page': 45}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4 \\n5\\n6\\n8\\n9\\n10\\n11\\n12\\n13```\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Beseye Pro, belonging to category: Cloud-Based Home Security \\nCamera\\n### Response:\\nBeseye Pro combines intelligent home monitoring with decorative art. The camera, whose form is reminiscent of a water \\ndrop, is secured in the mounting with a neodymium magnet and can be rotated by 360 degrees. This allows it to be \\neasily positioned in the desired direction. The camera also houses modern technologies, such as infrared LEDs, cloud-\\nbased intelligent video analyses and SSL encryption.\\n### End\\n```\\nTo facilitate quick experimentation, each fine-tuning exercise will be done on a 5000 observation subset  \\nof this data.\\nTESTING MODEL PERFORMANCE BEFORE FINE-TUNING\\nBefore any fine-tuning, it’s a good idea to check how the model performs without any fine-tuning to get a \\nbaseline for pretrained model performance.\\nThe model can be loaded in 8-bit as follows and prompted with the format specified in the model card on \\nHugging Face .47', metadata={'source': 'genai_notes.pdf', 'page': 46}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15import torch\\nfrom transformers import LlamaTokenizer, LlamaForCausalLM\\nmodel_path = \\'openlm-research/open_llama_3b_v2\\'\\ntokenizer = LlamaTokenizer.from_pretrained(model_path)\\nmodel = LlamaForCausalLM.from_pretrained(\\nmodel_path, load_in_8bit=True, device_map=\\'auto\\',\\n)\\n#Pass in a prompt and infer with the model\\nprompt = \\'Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: \\nOptical Mouse\\\\nA:\\'\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6Q: Create a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical \\nMouse A: The Corelogic Smooth Mouse is a wireless optical mouse that has a 1000 dpi resolution. It has a 2.4 GHz \\nwireless connection and a 12-month warranty. Q: What is the price of the Corelogic Smooth Mouse? A: The Corelogic \\nSmooth Mouse is priced at $29.99. Q: What is the weight of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse \\nweighs 0.1 pounds. Q: What is the dimensions of the Corelogic Smooth Mouse? A: The Corelogic Smooth Mouse has a \\ndimensionThe output obtained is not quite what we want.\\nThe first part of the result is actually satisfactory, but the rest of it is more of a rambling mess.48', metadata={'source': 'genai_notes.pdf', 'page': 47}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9prompt= \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n### Instruction:\\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category: Optical Mouse\\n### Response:\"\"\"\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\ngeneration_output = model.generate(\\ninput_ids=input_ids, max_new_tokens=128\\n)\\nprint(tokenizer.decode(generation_output[0]))\\n \\n1\\n2\\n3\\n4 \\n5\\n6Corelogic Smooth Mouse is a mouse that is designed to be used by people with disabilities. It is a wireless mouse \\nthat is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by people \\nwith disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a wireless \\nmouse that is designed to be used by people with disabilities. It is a wireless mouse that is designed to be used by \\npeople with disabilities. It is a wireless mouse that is designed to be used by people with disabilities. It is a \\nwireless mouse that is designed to be used bySimilarly, if the model is prompted with the input text in the ‘Alpaca format’ as discussed before, the output is \\nexpected to be just as suboptimal:\\nAnd sure enough, it is:\\nThe model performs what it was trained to do, predicts the next most probable token. The point of supervised \\nfine-tuning in this context is to generate the desired text in a controllable manner. Please note that in the \\nsubsequent experiments, while QLoRA leverages a model loaded in 4-bit with the weights frozen, the  \\ninference process to examine output quality is done once the model has been loaded in 8-bit as shown  \\nabove for consistency.49', metadata={'source': 'genai_notes.pdf', 'page': 48}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AITHE TURNABLE KNOBS\\nWhen using PEFT to train a model with LoRA or QLoRA (note that, as mentioned before, the primary difference \\nbetween the two is that in the latter, the pretrained models are frozen in 4-bit during the fine-tuning process), \\nthe hyperparameters of the low rank adaptation process can be defined in a LoRA config as shown below:\\nTwo of these hyperparameters, r and target_modules are empirically shown to affect adaptation quality \\nsignificantly and will be the focus of the tests that follow. The other hyperparameters are kept constant at the \\nvalues indicated above for simplicity.\\nr represents the rank of the low rank matrices learned during the fine-tuning process. As this value is increased, \\nthe number of parameters needed to be updated during the low-rank adaptation increases. Intuitively, a lower \\nr may lead to a quicker, less computationally intensive training process, but may affect the quality of the model \\nthus produced. However, increasing r beyond a certain value may not yield any discernible increase in quality of \\nmodel output. How the value of r affects adaptation (fine-tuning) quality will be put to the test shortly. \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14from peft import LoraConfig\\n...\\n...\\n#If only targeting attention blocks of the model\\ntarget_modules = [\"q_proj\", \"v_proj\"]\\n#If targeting all linear layers\\ntarget_modules = [\\'q_proj\\',\\'k_proj\\',\\'v_proj\\',\\'o_proj\\',\\'gate_proj\\',\\'down_proj\\',\\'up_proj\\',\\'lm_head\\']\\nlora_config = LoraConfig(\\nr=16,\\ntarget_modules = target_modules,\\nlora_alpha=8,\\nlora_dropout=0.05,\\nbias=\"none\",\\ntask_type=\"CAUSAL_LM\",}50', metadata={'source': 'genai_notes.pdf', 'page': 49}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIWhen fine-tuning with LoRA, it is possible to target specific modules in the model architecture. The adaptation \\nprocess will target these modules and apply the update matrices to them. Similar to the situation with \"r,\" \\ntargeting more modules during LoRA adaptation results in increased training time and greater demand for \\ncompute resources. Thus, it is a common practice to only target the attention blocks of the transformer. \\nHowever, recent work as shown in the QLoRA paper  by Dettmers et al. suggests that targeting all linear layers \\nresults in better adaptation quality. This will be explored here as well.\\nNames of the linear layers of the model can be conveniently appended to a list with the following code snippet:\\nTUNING THE FINE-TUNING WITH LORA\\nThe developer experience of fine-tuning large language models in general have improved dramatically over the \\npast year or so. The latest high level abstraction from Hugging Face is the SFTTrainer class in the TRL library. To \\nperform QLoRA, all that is needed is the following:\\n1. Load the model to GPU memory in 4-bit (bitsandbytes enables this process)\\n2. Define the LoRA configuration as discussed previously\\n3. Define the train and test splits of the prepped instruction following data into Hugging Face  \\nDataset objects\\n4. Define training arguments: These include the number of epochs, batch size and other training \\nhyperparameters which will be kept constant during this exercise\\n5. Pass these arguments into an instance of SFTTrainer\\nThese steps are clearly indicated in the source file in the repository  associated with this blog. \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9import re\\nmodel_modules = str(model.modules)\\npattern = r\\'\\\\((\\\\w+)\\\\): Linear\\'\\nlinear_layer_names = re.findall(pattern, model_modules)\\nnames = []\\n# Print the names of the Linear layers\\nfor name in linear_layer_names:\\n    names.append(name)\\ntarget_modules = list(set(names))51', metadata={'source': 'genai_notes.pdf', 'page': 50}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe actual training logic is abstracted away nicely as follows:\\nIf MLflow autologging is enabled in the Databricks workspace, which is highly recommended, all the training \\nparameters and metrics are automatically tracked and logged with the MLflow tracking server. This functionality \\nis invaluable in monitoring long-running training tasks. Needless to say, the fine-tuning process is performed \\nusing a compute cluster (in this case, a single node with a single A100 GPU) created using the latest Databricks \\nMachine Runtime with GPU support. \\n1\\n2\\n3\\n4 \\n5\\n6\\n7\\n8\\n9\\n10\\n11trainer = SFTTrainer(\\nmodel,\\ntrain_dataset=dataset[\\'train\\'],\\neval_dataset = dataset[\\'test\\'],\\ndataset_text_field=\"text\",\\nmax_seq_length=256,\\nargs=training_args,\\n)\\n# Initiate the training process\\nwith mlflow.start_run(run_name= ‘run_name_of_choice’):\\ntrainer.train()\\n52', metadata={'source': 'genai_notes.pdf', 'page': 51}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #1: QLoRA with r=8 and targeting “q_proj”, “v_proj”\\nThe first combination of QLoRA hyperparameters attempted is r=8 and targets only the attention blocks, namely \\n“q_proj” and “v_proj” for adaptation.\\nThe following code snippets gives the number of trainable parameters:\\nThese choices result in 2,662,400 parameters being updated during the fine-tuning process (~2.6 million) from a \\ntotal of ~3.2 billion parameters the model consists of. This is less than 0.1% of the model parameters. The entire \\nfine-tuning process on a single Nvidia A100 with 80 GBs of GPU for 3 epochs only takes roughly 12 minutes. The \\nGPU utilization metrics can be conveniently viewed at the metrics tab of the cluster configurations. \\n1\\n2model = get_peft_model(model, lora_config)\\nmodel.print_trainable_parameters()\\n53', metadata={'source': 'genai_notes.pdf', 'page': 52}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIAt the end of the training process, the fine-tuned model is obtained by loading the adapter weights to the \\npretrained model as follows:\\nThis model can now be used for inference as any other model.\\nQualitative Evaluation  \\nA couple of example prompt-response pairs are listed below\\nPrompt (passed to the model in the Alpaca format, not shown for conciseness here):  \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt:  \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe model has clearly been adapted for generating more consistent descriptions. However the response to the \\nfirst prompt about the optical mouse is quite short and the following phrase “ The vacuum cleaner is equipped \\nwith a dust container that can be emptied via a dust container ” is logically flawed. \\n1peft_model = PeftModel.from_pretrained(model, adapter_location)\\n \\n1\\n2The Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4The Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a turbo brush. The brush is suitable \\nfor cleaning carpets and hard floors. The turbo brush is suitable for cleaning carpets and hard floors. The vacuum \\ncleaner is equipped with a dust container that can be emptied via a dust container.54', metadata={'source': 'genai_notes.pdf', 'page': 53}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #2: QLoRA with r=16 and targeting all linear layers\\nSurely, things can be improved here. It is worth exploring increasing the rank of low rank matrices learned during \\nadaptation to 16, i.e., double the value of r to 16 and keep all else  the same. This doubles the number of trainable \\nparameters to 5,324,800 (~5.3 million)\\nQualitative Evaluation  \\nThe quality of output, however, remains unchanged for the same exact prompts.\\nPrompt:  \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\n \\n1\\n2The Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in black and white.\\n \\n1\\n2\\n3\\n4\\n5The Hoover Lightspeed is a cordless vacuum cleaner that is equipped with a lithium-ion battery. The battery is \\ncharged via a USB cable. The vacuum cleaner is equipped with a 2-in-1 brush and a motorized brush. The brush is used \\nto clean hard floors and the motorized brush is used to clean carpets. The vacuum cleaner is equipped with a dust \\ncontainer that can be emptied via a dust container. The vacuum cleaner is equipped with a LED display that shows the \\nremaining battery capacity.Prompt:  \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nThe same lack of detail and logical flaws in detail where details are available persists. If this fine tuned model is \\nused for product description generation in a real-world scenario, this is not acceptable output.55', metadata={'source': 'genai_notes.pdf', 'page': 54}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #3: QLoRA with r=8 and targeting all linear layers\\nGiven that doubling r does not seemingly result in any perceivable increase in output quality, it is worth \\nchanging the other important knob. i.e., targeting all linear layers instead of just the attention blocks. Here, the \\nLoRA hyperparameters are r=8 and target_layers are  'q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_\\nproj' and 'lm_head'. This increases the number of parameters updated to 12,994,560 and increases the training \\ntime to roughly 15.5 minutes.\\nQualitative Evaluation  \\nPrompting the model with the same prompts yield the following:\\nPrompt:  \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt:  \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nNow it is possible to see a somewhat longer coherent description of the fictitious optical mouse and there are \\nno logical flaws in the description of the vacuum cleaner. The product descriptions are not only logical, but \\nrelevant. Just as a reminder, these relatively high-quality results are obtained by fine-tuning less than a 1% of the \\nmodel’s weights with a total dataset of 5000 such prompt-description pairs formatted in a consistent manner. \\n1\\n2\\n3The Corelogic Smooth Mouse is a wireless optical mouse with a smooth surface. The mouse is equipped with a 1000 DPI \\nsensor and a 1000 Hz polling rate. The mouse is equipped with a 2.4 GHz wireless connection and a USB receiver. The \\nmouse is available in black and white.\\n \\n1\\n2\\n3\\n4The Hoover Lightspeed cordless vacuum cleaner is a high-performance device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of runtime. The device is equipped with a \\nhigh-performance motor that provides a suction power of up to 200 watts. The device is equipped with a 2-in-1 dusting \\nbrush and a crevice tool. The device is equipped with a LED display that shows the remaining battery charge.56\", metadata={'source': 'genai_notes.pdf', 'page': 55}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIHYPERPARAMETER COMBINATION #4: LoRA with  r=8 and targeting all linear transformer layers\\nIt is also worth exploring whether the quality of output from the model improves if the pretrained model is \\nfrozen in 8-bit instead of 4-bit. In other words, replicating the exact fine-tuning process using LoRA instead of \\nQLoRA. Here, the LoRA hyperparameters are kept the same as before, in the new-found optimal configuration, \\ni.e., r=8 and targeting all linear transformer layers during the adaptation process.\\nQualitative Evaluation  \\nThe results for the two prompts used throughout the article are as given below:\\nPrompt:  \\nCreate a detailed description for the following product: Corelogic Smooth Mouse, belonging to category:  \\nOptical Mouse\\nResponse:\\nPrompt:  \\nCreate a detailed description for the following product: Hoover Lightspeed, belonging to category: Cordless \\nVacuum Cleaner\\nResponse:\\nAgain, there isn’t much of an improvement in the quality of the output text.  \\n1\\n2\\n3The Corelogic Smooth Mouse is a high-quality optical mouse with a smooth surface. The mouse is equipped with a 1000 \\nDPI sensor and a 1000 Hz polling rate. The mouse is available in three colours: black, white and red. The mouse is \\nequipped with a USB receiver and is compatible with Windows and Mac.\\n \\n1\\n2\\n3 \\n4The Hoover Lightspeed cordless vacuum cleaner is a compact and lightweight device that is easy to use. The device is \\nequipped with a lithium-ion battery that provides up to 40 minutes of cleaning time. The vacuum cleaner is equipped \\nwith a high-performance filter that ensures that the air is cleaned of dust and allergens. The device is equipped \\nwith a 2-in-1 dusting brush and a crevice tool that can be used to clean hard-to-reach areas.57', metadata={'source': 'genai_notes.pdf', 'page': 56}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIKEY OBSERVATIONS\\nBased on the above set of trials, and further evidence detailed in the excellent publication presenting QLoRA, \\nit can be deduced that the value of r (the rank of matrices updated during adaptation) does not improve \\nadaptation quality beyond a certain point. The biggest improvement is observed in targeting all linear layers \\nin the adaptation process, as opposed to just the attention blocks, as commonly documented in technical \\nliterature detailing LoRA and QLoRA. The trials executed above and other empirical evidence suggest that \\nQLoRA does not indeed suffer from any discernible reduction in quality of text generated, compared to LoRA.\\nFURTHER CONSIDERATIONS FOR USING LORA ADAPTERS IN DEPLOYMENT\\nIt's important to optimize the usage of adapters and understand the limitations of the technique. The size of the \\nLoRA adapter obtained through fine-tuning is typically just a few megabytes, while the pretrained base model \\ncan be several gigabytes in memory and on disk. During inference, both the adapter and the pretrained LLM \\nneed to be loaded, so the memory requirement remains similar.\\nFurthermore, if the weights of the pre-trained LLM and the adapter aren’t merged, there will be a slight increase \\nin inference latency. Fortunately, with the PEFT library, the process of merging the weights with the adapter can \\nbe done with a single line of code as shown here:\\nThe figure below outlines the process from fine-tuning an adapter to model deployment. \\n1merged_model = peft_model.merge_and_unload()\\n58\", metadata={'source': 'genai_notes.pdf', 'page': 57}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIWhile the adapter pattern offers significant benefits, merging adapters is not a universal solution. One \\nadvantage of the adapter pattern is the ability to deploy a single large pretrained model with task-specific \\nadapters. This allows for efficient inference by utilizing the pretrained model as a backbone for different \\ntasks. However, merging weights makes this approach impossible. The decision to merge weights depends on \\nthe specific use case and acceptable inference latency. Nonetheless, LoRA/ QLoRA continues to be a highly \\neffective method for parameter efficient fine-tuning and is widely used.\\nCONCLUSION\\nLow Rank Adaptation is a powerful fine-tuning technique that can yield great results if used with the right \\nconfiguration. Choosing the correct value of rank and the layers of the neural network architecture to target \\nduring adaptation could decide the quality of the output from the fine-tuned model. QLoRA results in further \\nmemory savings while preserving the adaptation quality. Even when the fine-tuning is performed,  there are \\nseveral important engineering considerations to ensure the adapted model is deployed in the correct manner.\\nIn summary, a concise table indicating the different combinations of LoRA parameters attempted, text quality \\noutput and number of parameters updated when fine-tuning OpenLLaMA-3b-v2 for 3 epochs on 5000 \\nobservations on a single A100 is shown below.\\nTry this on Databricks! Clone the GitHub repository  associated with the blog into a Databricks Repo  to get \\nstarted. More thoroughly documented examples to fine-tune models on Databricks are available here .R TARGET_MODULESBASE MODEL \\nWEIGHTSQUALITY OF OUTPUTNUMBER OF PARAMETERS UPDATED  \\n(IN MILLIONS)\\n8 Attention blocks 4 low 2.662\\n16 Attention blocks 4 low 5.324\\n8 All linear layers 4 high 12.995\\n8 All linear layers 8 high 12.99559', metadata={'source': 'genai_notes.pdf', 'page': 58}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIStage 4: Pretraining\\nPretraining a model from scratch refers to the process of training a language model on a large corpus of data \\n(e.g., text, code) without using any prior knowledge or weights from an existing model. This is in contrast to fine-\\ntuning, where an already pretrained model is further adapted to a specific task or dataset. The output of full \\npretraining is a base model that can be directly used or further fine-tuned for downstream tasks.\\nWHEN TO USE PRETRAINING\\nChoosing to pretrain an LLM from scratch is a significant commitment, both in terms of data and computational \\nresources. Here are some scenarios where it makes sense:\\n1. Unique data sources:  If you possess a unique and extensive corpus of data that is distinct from what \\navailable pretrained LLMs have seen, it might be worth pretraining a model to capture this uniqueness\\n2. Domain specificity:  Organizations might want a base model tailored to their specific domain (e.g., \\nmedical, legal, code) to ensure even the foundational knowledge of the model is domain-specific\\n3. Full control over training data: Pretraining from scratch offers transparency and control over the data \\nthe model is trained on. This may be essential for ensuring data security, privacy and custom tailoring of \\nthe model’s foundational knowledge.\\n4. Avoiding third-party biases:  Pretraining ensures that your LLM application does not inherit biases or \\nlimitations from third-party pretrained models.60', metadata={'source': 'genai_notes.pdf', 'page': 59}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIPRETRAINING IN PRACTICE\\nGiven the resource-intensive nature of pretraining, careful planning and sophisticated tooling are required. \\nLibraries like PyTorch FSDP  and Deepspeed , mentioned in the fine-tuning section , are similarly required for \\ntheir distributed training capabilities when pretraining an LLM from scratch. The following only scratches the \\nsurface on some of the considerations one must take into account when pretraining an LLM: \\n ■Large-scale data preprocessing:  A pretrained model is only as good as the data it is trained on. Thus, \\nit becomes vitally important to ensure robust data preprocessing is conducted prior to model training. \\nGiven the scale of the training data involved, this preprocessing typically requires distributed frameworks \\nlike Apache Spark ™. Consideration must be given to factors such as dataset mix and deduplication \\ntechniques to ensure the model is exposed to a wide variety of unique data points.\\n ■Hyperparameter selection and tuning: Before executing full-scale training of an LLM, determining \\nthe set of optimal hyperparameters is crucial. Given the high computational cost associated with LLM \\ntraining, extensive hyperparameter sweeps are not always feasible. Instead, informed decisions based \\non smaller-scale searches or prior research are employed. Once a promising set is identified, these \\nhyperparameters are used for the full training run. Tooling like MLflow  is essential to manage and track \\nthese experiments.\\n ■Maximizing resource utilization: Given the high costs associated with long-running distributed GPU \\ntraining jobs, it is hugely important to maximize resource utilization. MosaicML’s composer  is an example \\nof a library that uses PyTorch FSDP  with additional optimizations to maximize Model FLOPs Utilization \\n(MFU) and Hardware FLOPs Utilization (HFU)  during training.\\n ■Handling GPU failures:  Training large models can run for days or even weeks. During such large-scale \\ntraining for this length of time, hardware failures, especially GPU failures, can (and typically do) occur.  \\nIt is essential to have mechanisms in place to handle such failures gracefully. \\n ■Monitoring and evaluation:  Close monitoring of the training process is essential. Saving model \\ncheckpoints regularly and evaluating validation sets not only act as safeguards but also provide insights \\ninto model performance and convergence trends.\\nIn cases where pretraining an LLM from scratch is required, Mosaic AI Training  provides a platform to conduct \\ntraining of multibillion-parameter models in a highly optimized and automated manner. Automatically handling \\nGPU failures and resuming training without human intervention and leveraging Mosaic AI Streaming  for efficient \\nstreaming of data into the training process are just some of the capabilities provided out of the box.61', metadata={'source': 'genai_notes.pdf', 'page': 60}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe Value of Training Models From Scratch on Databricks  \\nAfter diving into the details of starting a model’s training from scratch, why you might do it and the advanced \\ntools needed, let’s look at a real-world example to show that training top-notch language models isn’t as \\ncomplex or expensive as it might seem. This shift highlights that even organizations watching their budget can \\nstart training their own models, with Databricks providing the necessary support and infrastructure. Databricks \\nstands out as uniquely capable to help customers train their own models from scratch, enabling them to fully \\nown their AI assets.\\nPretraining Use Cases\\nTraining Stable Diffusion From Scratch for <$50K With MosaicML\\nby Mihir Patel , Cory Stephenson , Landan Seguin , Austin Jacobson  and Erica Ji Yuen\\nWe’ve replicated Stable Diffusion 2 for less than $50K, and we’ve open sourced the training code so you can \\ntoo! This is a 3x cost reduction from our last blog post and an 8x reduction from the original Stable Diffusion 2, \\nmaking training large-scale diffusion models from scratch more accessible than ever before.\\nToday, we are excited to show the results of our own training run: under $50K to train Stable Diffusion 2 base1 \\nfrom scratch in 7.45 days using the MosaicML platform .62', metadata={'source': 'genai_notes.pdf', 'page': 61}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AI\\nFigure 1: Imagining mycelium couture. Integrating image generation into the design process pushes creative boundaries. All images in this mood board \\nwere created with our internal diffusion model trained from scratch on the MosaicML Platform.\\nTraining your own image generation model on your own data is now easy and accessible. By training your own \\ndiffusion models, you can:\\n ■Use your proprietary data\\n ■Tune the representations for certain art or photography styles\\n ■Avoid violating intellectual property laws so your models can be used commercially\\nWe’ve open sourced our code and methods to train a diffusion model from scratch so that you can train your \\nown; check it out here ! If you're interested in training your own models, contact us for a demo , and read on to \\nlearn more about our engineering setup!63\", metadata={'source': 'genai_notes.pdf', 'page': 62}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AI\\nSETUP\\nModel:  Our diffusion model is a ComposerModel  \\ncomposed of a Variational Autoencoder (VAE), a \\nCLIP model, a U-Net, and a diffusion noise scheduler, \\nall from the HuggingFace's Diffusers library. All of \\nthe model configurations were based on stabilityai/\\nstable-diffusion-2-base .Figure 2: Getting creative and embracing serendipity. A variety of subjects, art, and photography styles are generated by our diffusion model.\\nFigure 3: Simplified diagram of the diffusion model.\\n64\", metadata={'source': 'genai_notes.pdf', 'page': 63}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIData:  We trained on a subset of LAION-5B  that includes samples with English-only captions and an aesthetic \\nscore of 4.5+. Similar to Stable Diffusion 2 base, we did two phases of training based on the image resolution of \\nthe training data. For the first phase of training, we used all images with resolution >=256x256, amounting to 790 \\nmillion image-caption samples. For the second phase of training, we only used images with resolution >=512x512, \\namounting to 300 million image-caption samples.\\nCompute: Both phases of training ran on 128 NVIDIA A100 GPUs. The first training phase was run for 550k \\niterations in 1.6 days while the second phase was run for 850k iterations in 4.9 days, for a total of 20,051 A100 \\nhours for training. In addition to the training time, we pre-computed the latents for the VAE and CLIP model \\nto reduce training time and cost when making multiple passes over the dataset. Pre-computing the latents \\nrequired an additional 3,784 A100 hours, resulting in 23,835 A100 hours in total. Assuming a cost of $2 / A100 \\nhour, the total price tag is $47.7k.\\nTech Stack:  We used Composer  for our training framework, StreamingDataset  to load our 100TB of data, and \\nthe MosaicML platform  for overcoming infrastructure challenges when training and evaluating on 128 GPUs.\\nFigure 4: Loss curve for our training run. Our platform caught two hardware failures and automatically restarted the run with no human intervention. \\nThe loss discontinuity is because phase 2 increases the resolution from 256x256 to 512x512.65', metadata={'source': 'genai_notes.pdf', 'page': 64}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AICHALLENGES AND SOLUTIONS\\nWhether for diffusion models or large language models, training at scale has significant challenges. We trained \\nour diffusion model using the MosaicML platform, which addresses these challenges automatically so you can \\nfocus on training the best possible model. Below are three main challenges with large-scale training and how our \\nplatform solves them.\\nINFRASTRUCTURE\\nTraining large models on large datasets requires significant compute. The MosaicML platform effortlessly \\norchestrates hundreds of GPUs on any cloud provider. For example, our primary training run took place on \\na cluster of 128 A100 GPUs. To ensure evaluating the model didn't slow training, we automatically kicked off \\nevaluation runs at every checkpoint on different clusters using different cloud providers, seamlessly scaling up \\nto 64 GPUs and back down to 8 GPUs depending on availability.\\nEven after training is underway, software or hardware failures can halt training, leaving GPUs idle until someone \\nnotices or requiring someone on-call 24/7 to babysit the run. Thankfully, the Node Doctor and Watchdog \\nfeatures of the MosaicML platform automatically detect failed nodes and resume jobs as needed. With auto-\\nresumption, we recover from failures and continue training with zero human intervention, avoiding expensive \\ndowntime and human babysitting. Just launch and train!\\nEFFICIENT SOFTWARE\\nSoftware is difficult to configure optimally. Our PyTorch-based Composer library  maximizes training efficiency \\nat scale. As shown in our previous blog post , Composer demonstrated excellent throughput scaling as the \\nnumber of GPUs increased. For this update, we added further optimizations (Low Precision GroupNorm  and Low \\nPrecision LayerNorm , Fully Sharded Data Parallel) to achieve near-perfect strong scaling up to 128 GPUs, bringing \\nthe cost down to $50k. We also used Composer's native Exponential Moving Average (EMA) algorithm, which \\nallowed us to start EMA close to the end of training (iteration 800k of the final phase) to gain all the benefits of \\nEMA while saving on memory and compute for the majority of training.66\", metadata={'source': 'genai_notes.pdf', 'page': 65}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIMANAGING 100TB OF DATA\\nWe trained with a subset of LAION-5B that contained 790 million samples, amounting to >100TB of data. The \\nsheer size of the dataset makes it difficult to manage, especially when working with multiple clusters with \\nseparate local storage. The MosaicML StreamingDataset library  makes working with massive datasets much \\nsimpler and faster. There were three key features of the StreamingDataset library that were especially useful for \\nthis training run:\\n1. Mixing datasets stored in different locations. We bucketed samples based on image resolution into \\ndifferent datasets. At training time, we used the MosaicML StreamingDataset library to train on a mixture \\nof resolutions from these datasets.\\n2. Instant mid-epoch resumption. We were able to instantly resume training in the middle of an epoch. This \\nsaved hours by avoiding the need to iterate over the entire dataset to get back to where we left off.\\n3. Elastic determinism. The MosaicML StreamingDataset library deterministically shuffles data, even when \\nchanging the number of GPUs used for training. This made it possible for us to exactly reproduce training \\nruns, dramatically simplifying debugging.\\nHUMAN EVALUATION RESULTS\\nEvaluating image generation models is difficult, and there is no substitute for human evaluation. In a blind human \\nevaluation, we measured user preferences in image quality and prompt alignment between Stable Diffusion 2 \\nand our diffusion model. Based on user preferences, we concluded that the two models were comparable in \\nquality (see Figure 5) All images were generated based on prompts from the Drawbench benchmark proposed \\nin the Imagen paper . For more details, see our follow-up blog post coming soon.67', metadata={'source': 'genai_notes.pdf', 'page': 66}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI\\nFigure 5: Results from our human evaluation of image quality (left) and prompt alignment (right). Error bars show 95% confidence intervals. In both ex -\\nperiments, the difference in user preference rates between the two models was comparable to the uncertainty in the measurement, so we conclude \\nthat the two models are of comparable overall quality.\\nDeep Dive: How We Trained Stable Diffusion for Less Than $50K  \\nby Mihir Patel , Erica Ji Yuen , Cory Stephenson  and Landan Seguin\\nIn our previous example, we showed how we used the MosaicML platform, Streaming datasets, and the \\nComposer library to train a Stable Diffusion model from scratch for less than $50,000. Now, we do a deep dive \\ninto the technical details behind this speedup, demonstrating how we were able to replicate the Stable Diffusion \\n2 base model in just 6.8 days.\\nTry out our code here !\\nMany organizations require high-performing large AI models tailored to their specific use cases. However, \\ntraining such models is often prohibitively time-consuming and expensive, requiring vast amounts of \\ncomputation and expertise. This is where MosaicML comes in: we provide a comprehensive solution that \\nsimplifies and accelerates the process of training these models.68', metadata={'source': 'genai_notes.pdf', 'page': 67}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIIn our previous blog post , we announced that we have trained a diffusion model comparable to Stable Diffusion \\n2 from scratch for $47.7K. In this post, we dive into the technical details to highlight how we achieved an 8x \\nspeedup/cost reduction from the number reported by StabilityAI  and a 3x cost reduction over our own \\nbaseline . All our code is open source  and easy to modify for custom use cases. If you're interested in learning \\nmore about our stack, please contact us for a demo .\\nACCELERATING TRAINING\\nWe’ve introduced a variety of techniques, from fusions to sharding strategies, that dramatically speed up \\ntraining and lower costs by almost 3x.\\nFigure 1: Stable Diffusion 2 model architecture. For training, the VAE image encoder, CLIP text encoder and U-Net are used. For inference,  \\nthe CLIP Text Encoder, U-Net, and VAE image decoder are used. Only the U-Net weights are updated during training; CLIP and VAE are fixed.69\", metadata={'source': 'genai_notes.pdf', 'page': 68}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIXFORMERS FLASHATTENTION\\nFigure 2: xFormers accelerates cross attention blocks in the U-Net.70', metadata={'source': 'genai_notes.pdf', 'page': 69}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe attention layers in the Stable Diffusion architecture can be slow with a naive implementation, so most \\ncodebases use faster implementations that rely on fused kernels. In our stack, we leverage xFormers \\nFlashAttention .\\nWhile this was enabled in our original blog post , we found an issue with the usage that resulted in extra memory \\nbeing consumed on rank 0. After fixing this bug, we were able to increase our device microbatch size1 from 4 to \\n8. This yielded a sizable speedup, since A100s are more efficient at larger matrix sizes.\\nPRECOMPUTING LATENTS\\nFigure 3: Two phase training with precomputed latents. \\nFirst, all VAE and CLIP latents are precomputed and stored. \\nThen, the U-Net diffusion model is trained using these \\nprecomputed latents.71', metadata={'source': 'genai_notes.pdf', 'page': 70}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIStable Diffusion is a combination of three models: a variational autoencoder (VAE), a text encoder (CLIP), and a \\nU-Net. During diffusion training, only the U-Net is trained, and the other two models are used to compute the \\nlatent encodings of the image and text inputs. Standard training involves computing the VAE and CLIP latents for \\nevery batch, but this does a lot of duplicate work when training for multiple epochs: latents are re-computed for \\neach image every time it is used. Instead, we precompute the latents once before training. Empirically, we have 2 \\nepochs at 256 resolution and 5 epochs at 512 resolution, so we avoid 6 extra VAE and CLIP calls per image-text \\npair in the dataset.\\nAdditionally, when pre-computing the latents, we can lower the precision of the VAE and CLIP models to \\nfp16. This could lead to numerical instability if we were training the VAE and CLIP and used this precision for \\nthe backward pass. However, since we're only using them for inference, we can safely lower the precision, \\nwhich increases speed. The extra memory savings also let us use far larger batch sizes and improve hardware \\nutilization during the latent precomputation.72\", metadata={'source': 'genai_notes.pdf', 'page': 71}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AILOW PRECISION LAYERNORM AND GROUPNORM\\nFigure 4: Low Precision LayerNorm and Low Precision GroupNorm. Low precision gives faster training and lower memory usage, enabling larger \\nmicrobatches.73', metadata={'source': 'genai_notes.pdf', 'page': 72}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIDiffusion training is done in automatic mixed precision  by default. This uses half precision (fp16) in most \\nlayers, but fp32 in a few numerically unstable layers like normalization and softmax. The Stable Diffusion U-Net \\narchitecture uses several LayerNorm and GroupNorm layers, which by default are run in fp32.\\nMotivated by our finding that half precision LayerNorms are safe to use in language models , we decided to \\ntry out half precision LayerNorm and GroupNorm layers. This change resulted in identical loss curves and no \\ninstability in our experiments.\\nWhile we did observe some throughput improvement, the real benefit was decreased memory usage. Now, \\nalong with removing the VAE and CLIP memory by precomputing latents, we have enough space on our 40GB \\nA100 to increase our microbatch size from 8 to 16, 4x larger than what we started with!74', metadata={'source': 'genai_notes.pdf', 'page': 73}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIFULLY SHARDED DATA PARALLELISM\\nFigure 5: Fully Sharded Data Parallel with SHARD_GRAD_OP speeds up the gradient update step and enables linear scaling.75', metadata={'source': 'genai_notes.pdf', 'page': 74}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIMosaicML Composer , our go-to training library, includes support for PyTorch Fully Sharded Data Parallelism \\n(FSDP). We primarily use this to shard large scale models like 10B+ parameter LLMs that don't fit in a single \\ndevice across hundreds of GPUs for incredibly fast training. Stable Diffusion doesn't require sharding since it  \\nfits in a single GPU. However, some of the distributed features in FSDP are still useful for speeding up training  \\non a large number of GPUs.\\nWhen batches don’t fit into memory, we do several forward and backward passes on smaller microbatches, \\nfollowed by a single gradient update. If we use a small number of GPUs to train, we have far more forward and \\nbackward passes per gradient update, so the time spent on the gradient update doesn't matter. However, at \\n128+ GPUs with a microbatch size of 16, we're only doing one forward and one backward pass for each gradient \\nupdate. At this scale, the gradient update step starts to become a significant bottleneck.\\nTo tackle this problem, we use FSDP's SHARD_GRAD_OP mode. In normal training, each GPU communicates all \\nits gradients to every other GPU, and then each GPU updates its local copy of the model. With this FSDP variant, \\neach GPU only gets the gradients and updates the weights for a small part of the model before sending the \\nupdated weights for that part of the model to all of the other GPUs. By dividing the update step across all the \\nGPUs, we can ensure the amount of work per GPU decreases as we increase the number of GPUs, helping us \\nachieve linear scaling.76\", metadata={'source': 'genai_notes.pdf', 'page': 75}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AISCHEDULED EMA\\nFigure 6: Loss curve of our training run with the scheduled exponential moving average (EMA) period highlighted.77', metadata={'source': 'genai_notes.pdf', 'page': 76}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIStable Diffusion 2 uses Exponential Moving Averaging (EMA) , which maintains an exponential moving average \\nof the weights. At every time step, the EMA model is updated by taking 0.9999 times the current EMA model \\nplus 0.0001 times the new weights after the latest forward and backward pass. By default, the EMA algorithm is \\napplied after every gradient update for the entire training period. However, this can be slow due to the memory \\noperations required to read and write all the weights at every step.\\nTo avoid this costly procedure, we start with a key observation: since the old weights are decayed by a factor of \\n0.9999 at every batch, the early iterations of training only contribute minimally to the final average. This means \\nwe only need to take the exponential moving average of the final few steps. Concretely, we train for 1,400,000 \\nbatches and only apply EMA for the final 50,000 steps, which is about 3.5% of the training period. The weights \\nfrom the first 1,350,000 iterations decay away by (0.9999)^50000, so their aggregate contribution would have \\na weight of less than 1% in the final model. Using this technique, we can avoid adding overhead for 96.5% of \\ntraining and still achieve a nearly equivalent EMA model.78', metadata={'source': 'genai_notes.pdf', 'page': 77}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIFINAL TIME AND COST ESTIMATES\\nFigure 7: Throughput at 512x512 images on 128 GPUs as each speedup optimization is enabled. We achieve a total cumulative speedup of 2.71x over \\nthe baseline.\\nWe’ve shown how we obtained nearly a 3x reduction in time and cost to train Stable Diffusion compared to our \\noriginal results . With xFormers, precomputed latents, low precision LayerNorm, low precision GroupNorm, FSDP, \\nand scheduled EMA, Table 1 shows it's possible to train Stable Diffusion in just 6.79 days using 21,000 A100-\\nhours for a total cost of less than $42,000. We estimated these times and costs by measuring throughput for \\ntraining 1.1 billion 256x256 images and 1.7 billion 512x512 images with a max tokenized length of 77 at a global \\nbatch size of 2048, as detailed in the Stable Diffusion 2 base model card . This is slightly cheaper than our \\npreviously reported run  with a cost of $47.7k as it does not account for any time spent on evaluation or restarts \\ndue to hardware failures.79\", metadata={'source': 'genai_notes.pdf', 'page': 78}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AINUMBER  \\nOF A100STHROUGHPUT  \\nFOR U-NET \\n@ 256X256 \\n(IMAGES / \\nSECOND)THROUGHPUT \\nFOR U-NET \\n@ 512X512 \\n(IMAGES / \\nSECOND)THROUGHPUT \\nFOR U-NET @ \\n512X512 WITH \\nEMA (IMAGES /  \\nSECOND)DAYS TO TRAIN \\nON MOSAICML \\nCLOUDAPPROX. COST \\nON MOSAICML \\nCLOUD\\n8 1100 290 290 101.04 $38,800\\n16 2180 585 580 50.29 $38,630\\n32 4080 1195 1160 25.01 $38,420\\n64 8530 2340 2220 12.63 $38,800\\n128 11600 4590 3927 6.79 $41,710\\nTable 1: Estimated time and cost to train a Stable Diffusion model on 1.1 billion images at 256x256 resolution, followed by 1.7 billion images at 512x512 \\nresolution. Different rows show different numbers of NVIDIA 40GB A100 GPUs at a global batch size of 2048.\\nThese optimizations show that training image generation models from scratch is within reach for everyone. For \\nupdates on our latest work, join our Community Slack  or follow us on Twitter . If your organization wants to start \\ntraining diffusion models today, please schedule a demo online  or email us at demo@mosaicml.com .\\n1 When training large models with big batches that don't fit in memory in a single pass, each batch is divided into smaller microbatches. On each \\ndevice, we can do a forward and backward pass for each microbatch and sum the gradients at the end to compute a gradient update equivalent to \\na single forward and backward pass with the entire batch all at once.80\", metadata={'source': 'genai_notes.pdf', 'page': 79}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIStage 5: LLM Evaluation\\nConstant evaluation and monitoring of deployed large language models (LLMs) and generative AI applications \\nare crucial due to the dynamic nature of both the data they interact with and the environments in which they \\noperate. These systems learn from vast datasets and can evolve over time, potentially leading to shifts in \\nperformance, accuracy or even the emergence of biases. Continuous monitoring ensures that any deviation \\nfrom expected behavior can be detected and corrected promptly, maintaining the integrity and reliability \\nof the AI application. As user needs and societal norms change, ongoing evaluation  allows these models to \\nadapt, ensuring their outputs remain relevant, appropriate and effective. This vigilance not only mitigates risks \\nassociated with AI deployments, such as ethical concerns and regulatory compliance, but also maximizes the \\nvalue and utility these technologies bring to organizations and end users. \\nEvaluating LLMs is a challenging and evolving domain , primarily because LLMs often demonstrate uneven \\ncapabilities across different tasks. An LLM might excel in one benchmark, but slight variations in the prompt \\nor problem can drastically affect its performance. The dynamic nature of LLMs and their vast potential \\napplications only amplify the challenge of establishing comprehensive evaluation standards.81', metadata={'source': 'genai_notes.pdf', 'page': 80}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIPresent challenges involved with evaluating LLM-powered applications include the following:\\n ■Variable performance:  LLMs can be sensitive to prompt variations , demonstrating high proficiency in \\none task but faltering with slight deviations in prompts.\\n ■Lack of ground truth:  Since most LLMs output natural language, it is very difficult to evaluate the outputs \\nvia traditional NLP metrics ( BLEU , ROUGE, etc.). For example, suppose an LLM were used to summarize \\na news article. Two equally good summaries might have almost completely different words and word \\norders, so even defining a “ground truth” label becomes difficult or impossible.\\n ■Domain-specific evaluation:  For domain-specific fine-tuned LLMs, popular generic benchmarks \\nmay not capture their nuanced capabilities. Such models are tailored for specialized tasks, making \\ntraditional metrics less relevant. This divergence often necessitates the development of domain-specific \\nbenchmarks and evaluation criteria. See the example of Replit’s code generation LLM . \\n ■Reliance on human judgment:  It is often the case that LLM performance is being evaluated in domains \\nwhere text is scarce or there is a reliance on subject matter expert knowledge. In such scenarios, \\nevaluating LLM output can be costly and time-consuming.\\nTo help give examples of how this can be accomplished, here are two great examples of how you can monitor \\nand evaluate your deployed LLMs and generative AI applications using Databricks.\\nLLM Evaluation Examples\\nBest Practices for LLM Evaluation of RAG Applications   \\nA Case Study on the Databricks Documentation Bot\\nby Quinn Leng , Kasey Uhlenhuth  and Alkis Polyzotis\\nChatbots are the most widely adopted use case for leveraging the powerful chat and reasoning capabilities \\nof large language models (LLM). The retrieval augmented generation (RAG) architecture is quickly becoming \\nthe industry standard for developing chatbots because it combines the benefits of a knowledge base (via a \\nvector store) and generative models (e.g., GPT-3.5 and GPT-4) to reduce hallucinations, maintain up-to-date \\ninformation, and leverage domain-specific knowledge. However, evaluating the quality of chatbot responses \\nremains an unsolved problem today. With no industry standards defined, organizations resort to human grading \\n(labeling) –which is time-consuming and hard to scale.82', metadata={'source': 'genai_notes.pdf', 'page': 81}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIWe applied theory to practice to help form best practices for LLM automated evaluation so you can deploy RAG \\napplications to production quickly and with confidence. This blog represents the first in a series of investigations \\nwe’re running at Databricks to provide learnings on LLM evaluation. All research in this post was conducted by \\nQuinn Leng, Senior Software Engineer at Databricks and creator of the Databricks Documentation AI Assistant . \\nCHALLENGES WITH AUTO-EVALUATION IN PRACTICE\\nRecently, the LLM community has been exploring the use of “LLMs as a judge” for automated evaluation with \\nmany using powerful LLMs such as GPT-4 to do the evaluation for their LLM outputs. The lmsys group’s research \\npaper explores the feasibility and pros/cons of using various LLMs (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, and world knowledge.\\nDespite all this great research, there are still many unanswered questions about how to apply LLM judges  \\nin practice:\\n ■Alignment With Human Grading: Specifically for a document-Q&A chatbot, how well does an \\nLLM judge’s grading reflect the actual human preference in terms of correctness, readability and \\ncomprehensiveness of the answers? \\n ■Accuracy Through Examples:  What’s the effectiveness of providing a few grading examples to the LLM \\njudge and how much does it increase the reliability and reusability of the LLM judge on different metrics?\\n ■Appropriate Grade Scales:  What grading scale is recommended because different grading scales are \\nused by different frameworks (e.g., AzureML  uses 0 to 100 whereas langchain  uses binary scales)?\\n ■Applicability Across Use Cases:  With the same evaluation metric (e.g. correctness), to what extent can \\nthe evaluation metric be reused across different use cases (e.g. casual chat, content summarization, \\nretrieval augmented generation)?83', metadata={'source': 'genai_notes.pdf', 'page': 82}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIAPPLYING EFFECTIVE AUTO-EVALUATION FOR RAG APPLICATIONS\\nWe explored the possible options for the questions outlined above in the context of our own chatbot \\napplication at Databricks. We believe that our findings generalize and can thus help your team effectively \\nevaluate RAG-based chatbots at a lower cost and faster speed:\\n ■LLM-as-a-judge agrees with human grading on over 80% of judgments. Using LLMs-as-a-judge for our \\ndocument-based chatbot evaluation was as effective as human judges, matching the exact score in over \\n80% of judgments and being within a 1-score distance (using a scale of 0-3) in over 95% of judgments.\\n ■Save costs by using GPT-3.5 with examples. GPT-3.5 can be used as an LLM judge if you provide \\nexamples for each grading score. Because of the context size limit it’s only practical to use a low-\\nprecision grading scale. Using GPT-3.5 with examples instead of GPT-4 drives down the cost of LLM judge \\nby 10x and improves the speed by more than 3x.\\n ■Use low-precision grading scales for easier interpretation. We found lower-precision grading scores like 0, \\n1, 2, 3 or even binary (0, 1) can largely retain precision compared to higher precision scales like 0 to 10.0 or \\n0 to 100.0, while making it considerably easier to provide grading rubrics to both human annotators and \\nLLM judges. Using a lower precision scale also allows consistency of grading scales among different LLM \\njudges (e.g., between GPT-4 and claude2).\\n ■RAG applications require their own benchmarks. A model might have good performance on a published \\nspecialized benchmark (e.g. casual chat, math, or creative writing) but that doesn’t guarantee good \\nperformance on other tasks (e.g. answering questions from a given context). Benchmarks should only be \\nused if the use case matches, i.e., a RAG application should only be evaluated with a RAG benchmark.\\nBased on our research, we recommend the following procedure when using an LLM judge: \\n ■Use a 1-5 grading scale\\n ■Use GPT-4 as an LLM judge with no examples to understand grading rules\\n ■Switch your LLM judge to GPT-3.5 with one example per score84', metadata={'source': 'genai_notes.pdf', 'page': 83}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIOUR METHODOLOGY FOR ESTABLISHING BEST PRACTICES\\nThe remainder of this post will walk through the series of experiments we conducted to form these  \\nbest practices. \\nEXPERIMENT SETUP\\n85', metadata={'source': 'genai_notes.pdf', 'page': 84}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI The experiment had three steps: \\n1. Generate evaluation dataset: We created a dataset from 100 questions and context from Databricks \\ndocuments. The context represents (chunks of) documents that are relevant to the question. \\n2. Generate answer sheets:  Using the evaluation dataset, we prompted different language models to \\ngenerate answers and stored the question-context-answer pairs in a dataset called “answer sheets”. In \\nthis investigation, we used GPT-4, GPT-3.5, Claude-v1, Llama2-70b-chat, Vicuna-33b, and mpt-30b-chat.\\n3. Generate grades:  Given the answer sheets, we used various LLMs to generate grades and reasoning \\nfor the grades. The grades are a composite score of Correctness (weighted: 60%), Comprehensiveness \\n(weighted: 20%) and Readability (weighted: 20%). We chose this weighting scheme to reflect our \\npreference for Correctness in the generated answers. Other applications may tune these weights \\ndifferently but we expect Correctness to remain a dominant factor.86', metadata={'source': 'genai_notes.pdf', 'page': 85}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIAdditionally, the following techniques were used to avoid positional bias and improve reliability:\\n ■Low temperature (temperature 0.1) to ensure reproducibility\\n ■Single-answer grading instead of pairwise comparison\\n ■Chain of thoughts to let the LLM reason about the grading process before giving the final score\\n ■Few-shots generation where the LLM is provided with several examples in the grading rubric for each \\nscore value on each factor (Correctness, Comprehensiveness, Readability)\\nEXPERIMENT 1: ALIGNMENT WITH HUMAN GRADING\\nTo confirm the level of agreement between human annotators and LLM judges, we sent answer sheets  \\n(grading scale 0-3) from gpt-3.5-turbo and vicuna-33b to a labeling company to collect human labels,  \\nand then compared the result with GPT-4’s grading output. Below are the findings:\\nHuman and GPT-4 judges can reach above 80% agreement on the correctness and readability score.  \\nAnd if we lower the requirement to be smaller or equal than 1 score difference, the agreement level can  \\nreach above 95%. The Comprehensiveness metric has less alignment, which matches what we’ve heard  \\nfrom business stakeholders who shared that “comprehensive” seems more subjective than metrics like \\nCorrectness or Readability.\\n87', metadata={'source': 'genai_notes.pdf', 'page': 86}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIEXPERIMENT 2: ACCURACY THROUGH EXAMPLES\\nThe lmsys paper uses this prompt  to instruct the LLM judge to evaluate based on the helpfulness, relevance, \\naccuracy, depth, creativity, and level of detail of the response. However, the paper doesn’t share specifics on the \\ngrading rubric. From our research, we found many factors can significantly affect the final score, for example:\\n ■The importance of different factors: Helpfulness, Relevance, Accuracy, Depth, Creativity\\n ■The interpretation of factors like Helpfulness is ambiguous \\n ■If different factors conflict with each other, where an answer is helpful but is not accurate \\nWe developed a rubric for instructing an LLM judge for a given grading scale, by trying the following:\\n1. Original Prompt:  Here is the original prompt used in the lmsys paper:\\nWe adapted the original lmsys paper prompt to emit our metrics about correctness, comprehensiveness and \\nreadability, and also prompt the judge to provide one line justification before giving each score (to benefit from \\nchain-of-thought reasoning). Below are the zero-shot version of the prompt which doesn’t provide any example, \\nand the few-shot version of the prompt which provides one example for each score. Then we used the same \\nanswer sheets as input and compared the graded results from the two prompt types.\\n2. Zero Shot Learning:  Require the LLM judge to emit our metrics about correctness, comprehensiveness \\nand readability, and also prompt the judge to provide one line justification for each score. \\nPlease act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question \\ndisplayed below. Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and \\nlevel of detail of the response. Begin your evaluation by providing a short explanation. Be as objective as possible. After \\nproviding your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context. \\n  You'll be given a function grading_function which you'll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer88\", metadata={'source': 'genai_notes.pdf', 'page': 87}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI3. Few Shots Learning:  We adapted the zero shot prompt to provide explicit examples for each score in the \\nscale. The new prompt:\\n \\nPlease act as an impartial judge and evaluate the quality of the provided answer which attempts to answer the provided \\nquestion based on a provided context.\\n  You\\'ll be given a function grading_function which you\\'ll call for each provided context, question and answer to submit your \\nreasoning and score for the correctness, comprehensiveness and readability of the answer. \\n  \\n  Below is your grading rubric: \\n- Correctness: If the answer correctly answer the question, below are the details for different scores:\\n  - Score 0: the answer is completely incorrect, doesn’t mention anything about the question or is completely contrary to the \\ncorrect answer.\\n      - For example, when asked “How to terminate a databricks cluster”, the answer is empty string, or content that’s \\ncompletely irrelevant, or sorry I don’t know the answer.\\n  - Score 1: the answer provides some relevance to the question and answers one aspect of the question correctly.\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: Databricks cluster is a cloud-based computing environment that allows users to process big data and run \\ndistributed data processing tasks efficiently.\\n          - Or answer:  In the Databricks workspace, navigate to the \"Clusters\" tab. And then this is a hard question that I \\nneed to think more about it\\n  - Score 2: the answer mostly answer the question but is missing or hallucinating on one critical aspect.\\n      - Example:\\n          - Question: How to terminate a databricks cluster”\\n          - Answer: “In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          And then you’ll find a button to terminate all clusters at once”\\n  - Score 3: the answer correctly answer the question and not missing any major aspect\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button. A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.”\\n- Comprehensiveness: How comprehensive is the answer, does it fully answer all aspects of the question and provide \\ncomprehensive explanation and other necessary information. Below are the details for different scores:\\n  - Score 0: typically if the answer is completely incorrect, then the comprehensiveness is also zero score.\\n  - Score 1: if the answer is correct but too short to fully answer the question, then we can give score 1 for \\ncomprehensiveness.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: First, you will need a Databricks access token with the appropriate permissions. You can generate this \\ntoken through the Databricks UI under the \\'User Settings\\' option. And then (the rest is missing)\\n  - Score 2: the answer is correct and roughly answer the main aspects of the question, but it’s missing description about \\ndetails. Or is completely missing details about one minor aspect.  89', metadata={'source': 'genai_notes.pdf', 'page': 88}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You will need a Databricks access token with the appropriate permissions. Then you’ll need to set up the \\nrequest URL, then you can make the HTTP Request. Then you can handle the request response.\\n  - Score 3: the answer is correct, and covers all the main aspects of the question\\n- Readability: How readable is the answer, does it have redundant information or incomplete information that hurts the \\nreadability of the answer.\\n  - Score 0: the answer is completely unreadable, e.g. fully of symbols that’s hard to read; e.g. keeps repeating the words \\nthat it’s very hard to understand the meaning of the paragraph. No meaningful information can be extracted from the answer.\\n  - Score 1: the answer is slightly readable, there are irrelevant symbols or repeated words, but it can roughly form a \\nmeaningful sentence that cover some aspects of the answer.\\n      - Example:\\n          - Question: How to use databricks API to create a cluster?\\n          - Answer: You you  you  you  you  you  will need a Databricks access token with the appropriate permissions. And \\nthen then you’ll need to set up the request URL, then you can make the HTTP Request. Then Then Then Then Then Then Then Then \\nThen\\n  - Score 2: the answer is correct and mostly readable, but there is one obvious piece that’s affecting the readability \\n(mentioning of irrelevant pieces, repeated words)\\n      - Example:\\n          - Question: How to terminate a databricks cluster\\n          - Answer: In the Databricks workspace, navigate to the \"Clusters\" tab.\\n          Find the cluster you want to terminate from the list of active clusters.\\n          Click on the down-arrow next to the cluster name to open the cluster details.\\n          Click on the \"Terminate\" button…………………………………..\\n          A confirmation dialog will appear. Click \"Terminate\" again to confirm the action.\\n  - Score 3: the answer is correct and reader friendly, no obvious piece that affect readability.\\n- Then final rating:\\n    - Ratio: 60% correctness + 20% comprehensiveness + 20% readability\\nFrom this experiment, we learned several things:\\n ■Using the Few Shots prompt with GPT-4 didn’t make an obvious difference in the consistency of results. \\nWhen we included the detailed grading rubric with examples we didn’t see a noticeable improvement in \\nGPT-4’s grading results across different LLM models. Interestingly, it caused a slight variance in the range \\nof the scores. 90', metadata={'source': 'genai_notes.pdf', 'page': 89}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI\\n91', metadata={'source': 'genai_notes.pdf', 'page': 90}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI ■Including few examples for GPT-3.5-turbo-16k significantly improves the consistency of the scores,  \\nand makes the result usable. Including detailed grading rubric/examples has very obvious improvement \\non the grading result from GPT-3.5. Though the actual average score value is slightly different between \\nGPT-4 and GPT-3.5 (score 3.0 vs score 2.6), the ranking and precision remains fairly consistent\\n ■On the contrary, using GPT-3.5 without a grading rubric gets very inconsistent results and is  \\ncompletely unusable\\n ■Note that we are using GPT-3.5-turbo-16k instead of GPT-3.5-turbo since the prompt can be larger than \\n4k tokens. \\n92', metadata={'source': 'genai_notes.pdf', 'page': 91}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI\\nEXPERIMENT 3: APPROPRIATE GRADE SCALES\\nThe LLM-as-judge paper uses a non-integer 0~10 scale (i.e., float) for the grading scale; in other words, it uses \\na high precision rubric for the final score. We found these high-precision scales cause issues downstream with \\nthe following:\\n ■Consistency:  Evaluators–both human and LLM–struggled to hold the same standard for the same score \\nwhen grading on high precision. As a result, we found that output scores are less consistent across judges \\nif you move from low-precision to high-precision scales. \\n ■Explainability: Additionally, if we want to cross-validate the LLM-judged results with human-judged \\nresults we must provide instructions on how to grade answers. It is very difficult to provide accurate \\ninstructions for each “score” in a high-precision grading scale–for example, what’s a good example for an \\nanswer that’s scored at 5.1 as compared to 5.6? 93', metadata={'source': 'genai_notes.pdf', 'page': 92}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIWe experimented with various low-precision grading scales to provide guidance on the “best” one to use, \\nultimately we recommend an integer scale of 0-3 or 0-4 (if you want to stick to the Likert  scale). We tried  \\n0-10, 1-5, 0-3, and 0-1 and learned:\\n ■Binary grading works for simple metrics like “usability” or “good/bad”.\\n ■Scales like 0-10 are difficult to come up with distinguishing criteria between all scores.\\n94', metadata={'source': 'genai_notes.pdf', 'page': 93}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI\\nAs shown in these plots, both GPT-4 and GPT-3.5 can retain consistent ranking of results using different  \\nlow-precision grading scales, thus using a lower grading scale like 0~3 or 1~5 can balance the precision  \\nwith explainability).\\nThus, we recommend 0-3 or 1-5 as a grading scale to make it easier to align with human labels, reason about \\nscoring criteria, and provide examples for each score in the range. 95', metadata={'source': 'genai_notes.pdf', 'page': 94}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIEXPERIMENT 4: APPLICABILITY ACROSS USE CASES\\nThe LLM-as-judge  paper shows that both LLM and human judgment ranks the Vicuna-13B model as a close \\ncompetitor to GPT-3.5:\\nHowever, when we benchmarked the set of models for our document Q&A use cases, we found that even the \\nmuch larger Vicuna-33B model has a noticeably worse performance than GPT-3.5 when answering questions \\nbased on context. These findings are also verified by GPT-4, GPT-3.5 and human judges (as mentioned in \\nExperiment 1) which all agree that Vicuna-33B is performing worse than GPT-3.5.Figure 4:  Average win rate of nine models under different judges on Chatbot Arena.\\n96', metadata={'source': 'genai_notes.pdf', 'page': 95}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI\\nWe looked closer at the benchmark dataset proposed by the paper and found that the 3 categories of tasks  \\n(writing, math, knowledge) don’t directly reflect or contribute to the model’s ability to synthesize an answer \\nbased on a context. Instead, intuitively, document Q&A use cases need benchmarks on reading comprehension \\nand instruction following. Thus evaluation results can’t be transferred between use cases and we need to build \\nuse-case-specific benchmarks in order to properly evaluate how good a model can meet customer needs.97', metadata={'source': 'genai_notes.pdf', 'page': 96}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIUSE MLFLOW TO LEVERAGE OUR BEST PRACTICES\\nWith the experiments above, we explored how different factors can significantly affect the evaluation of a \\nchatbot and confirmed that LLM as a judge can largely reflect human preferences for the document Q&A use \\ncase. At Databricks, we are evolving the MLflow Evaluation API to help your team effectively evaluate your LLM \\napplications based on these findings. MLflow 2.4 introduced the Evaluation API for LLMs to compare various \\nmodels’ text output side-by-side, MLflow 2.6 introduced LLM-based metrics for evaluation like toxicity and \\nperplexity, and we’re working to support LLM-as-a-judge in the near future!\\nIn the meantime, we compiled the list of resources we referenced in our research below:\\n ■Doc_qa repository\\n ■The code and data we used to conduct the experiments\\n ■LLM-as-Judge Research paper from lmsys group \\n ■The paper is the first research for using LLM as judge for the casual chat use cases, it extensively \\nexplored the feasibility and pros and cons of using LLM (GPT-4, ClaudeV1, GPT-3.5) as the judge for \\ntasks in writing, math, world knowledge\\nOffline LLM Evaluation: Step-by-Step GenAI Application Assessment on Databricks\\nby Abe Omorogbe , Liang Zhang , Sunish Sheth , Corey Zumar , Maheswaran Venkatachalam , Emil Lysgaard   \\nand Mathias Christiansen\\nBACKGROUND\\nIn an era where retrieval augmented generation (RAG) is revolutionizing the way we interact with AI-driven \\napplications, ensuring the efficiency and effectiveness of these systems has never been more essential. \\nDatabricks and MLflow are at the forefront of this innovation, offering streamlined solutions for the critical \\nevaluation of GenAI applications. \\nThis blog post guides you through the simple and effective process of leveraging the Databricks Data \\nIntelligence Platform to enhance and evaluate the quality of the three core components of your GenAI \\napplications: Prompts, Retrieval System, and Foundation LLM, ensuring that your GenAI applications continue  \\nto generate accurate results.98', metadata={'source': 'genai_notes.pdf', 'page': 97}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIUSE CASE\\nWe are going to be creating a QA chatbot that will answer questions from the MLflow documentation and then \\nevaluate the results.\\n99', metadata={'source': 'genai_notes.pdf', 'page': 98}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AISET UP EXTERNAL MODELS IN DATABRICKS\\nDatabricks Model Serving  feature can be used to manage, govern, and access external models from various \\nlarge language model (LLM) providers, such as Azure OpenAI GPT, Anthropic Claude, or AWS Bedrock, within \\nan organization. It offers a high-level interface that simplifies the interaction with these services by providing a \\nunified endpoint to handle specific LLM related requests.\\nMajor advantages of using Model Serving :\\n ■Query Models Through a Unified Interface:  Simplifies the interface to call multiple LLMs in your \\norganization. Query models through a unified OpenAI-compatible API and SDK and manage all models \\nthrough a single UI.\\n ■Govern and Manage Models:  Centralizes endpoint management of multiple LLMs in your organization.  \\nThis includes the ability to manage permissions and track usage limits.\\n ■Central Key Management: Centralizes API key management in a secure location, which enhances \\norganizational security by minimizing key exposure in the system and code, and reduces the burden  \\non end-users.100', metadata={'source': 'genai_notes.pdf', 'page': 99}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AICREATE A SERVING ENDPOINT WITH AN EXTERNAL MODEL IN DATABRICKS\\n \\n1\\n2\\n3 \\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n23\\n25\\n26import mlflow\\nimport mlflow.deployments\\nclient = mlflow.deployments.get_deploy_client(\"databricks\")\\nendpoint_name = f\"test-endpoint-{uuid.uuid4()}\"\\nclient.create_endpoint(\\nname=endpoint_name,\\nconfig={\\n        \"served_entities\": [\\n            {\\n                \"name\": \"test\",\\n                \"external_model\": {\\n                    \"name\": \"gpt-3.5-turbo-instruct\",\\n                    \"provider\": \"openai\",\\n                    \"task\": \"llm/v1/completions\",\\n                    \"openai_config\": {\\n                        \"openai_api_type\": \"azure\",\\n                        \"openai_api_key\": \"{{secrets/<your-scope-name>/<your-key-name>}}\", ## Use Databricks Secrets. \\n                        \"openai_api_base\": \"https://<your-endpoint>.openai.azure.com/\",\\n                        \"openai_deployment_name\": \"<your-deployment-name>\",\\n                        \"openai_api_version\": \"2023-05-15\",\\n                    },\\n                },\\n            }\\n        ],\\n     },\\n)101', metadata={'source': 'genai_notes.pdf', 'page': 100}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIEXPLORE PROMPTS WITH THE DATABRICKS AI PLAYGROUND\\nIn this section, we will understand: How well do different prompts perform with the chosen LLM?\\nWe recently introduced the Databricks AI Playground , which provides a best-in-class experience for crafting the \\nperfect prompt. With no code required, you can try out multiple LLMs served as Endpoints in Databricks, and \\ntest different parameters and prompts.\\nMajor advantages of the Databricks AI Playground are:\\n ■Quick Testing:  Quickly test deployed models directly in Databricks.\\n ■Easy Comparison:  Central location to compare multiple models on different prompts and parameters for \\ncomparison and selection.\\nUSING DATABRICKS AI PLAYGROUND\\nWe delve into testing relevant prompts with OpenAI GPT 3.5 Turbo, leveraging the Databricks AI Playground . 102', metadata={'source': 'genai_notes.pdf', 'page': 101}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AICOMPARING DIFFERENT PROMPTS AND PARAMETERS\\nIn the Playground, you are able to compare the output of multiple prompts to see which gives better results. \\nDirectly in the Playground, you can try several prompts,  models, and parameters to figure out which \\ncombination provides the best results. The model and parameters combo can then be added to the GenAI  \\napp and used for answer generation with the right context.\\n103', metadata={'source': 'genai_notes.pdf', 'page': 102}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIADDING MODEL AND PARAMETERS TO YOUR GENAI APPLICATION\\nAfter playing with a few prompts and parameters, you can use the same settings and model in your  \\nGenAI application.\\n104', metadata={'source': 'genai_notes.pdf', 'page': 103}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIExample of how to import the same external model in LangChain. We will cover how we turn this into a GenAI \\nPOC in the next section.\\nCREATE GENAI POC WITH LANGCHAIN AND LOG WITH MLFLOW\\nNow that we have found a good model and prompt parameters for your use case, we are going to create a \\nsample GenAI app that is a QA chatbot that will answer questions from the MLflow documentation using a \\nvector database, embedding model with the Databricks Foundation Model API  and Azure OpenAI GPT 3.5 as \\nthe generation model. \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9from langchain.llms import Databricks\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)105', metadata={'source': 'genai_notes.pdf', 'page': 104}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AICREATE A SAMPLE GENAI APP WITH LANGCHAIN USING DOCS FROM THE MLFLOW WEBSITE\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37import os\\nimport pandas as pd\\nimport mlflow\\nimport chromadb\\nfrom langchain.chains import RetrievalQA\\nfrom langchain.document_loaders import WebBaseLoader\\nfrom langchain.llms import Databricks\\nfrom langchain.embeddings.databricks import DatabricksEmbeddings\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\\nloader = WebBaseLoader(\\n    [ \\n     \"https://mlflow.org/docs/latest/index.html\",\\n     \"https://mlflow.org/docs/latest/tracking/autolog.html\", \\n     \"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\",\\n     \"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\" ])\\ndocuments = loader.load()\\nCHUNK_SIZE = 1000\\ntext_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\ntexts = text_splitter.split_documents(documents)\\nllm = Databricks(\\n    endpoint_name=\"<endpoint-name>\",\\n    extra_params={\"temperature\": 0.1,\\n                 \"top_p\": 0.1,\\n                 \"max_tokens\": 500,\\n                 } #parameters used in AI Playground\\n)\\n# create the embedding function using Databricks Foundation Model APIs\\nembedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\ndocsearch = Chroma.from_documents(texts, embedding_function)\\nqa = RetrievalQA.from_chain_type(\\n    llm=llm,\\n    chain_type=\"stuff\",\\n    retriever=docsearch.as_retriever(fetch_k=3),\\n    return_source_documents=True,\\n)106', metadata={'source': 'genai_notes.pdf', 'page': 105}),\n",
       " Document(page_content=\"THE BIG BOOK OF GENERATIVE AIFor customers wanting to scale the retriever used in their GenAI application, we advise using Databricks Vector \\nSearch, a serverless similarity search engine that allows you to store a vector representation of your data, \\nincluding metadata, in a vector database.\\nEVALUATION OF RETRIEVAL SYSTEM WITH MLFLOW\\nIn this section, we will understand: How well does the retriever work with a given query?\\nIn MLflow 2.9.1 , Evaluation for retrievers was introduced and provides a way for you to assess the efficiency \\nof their retriever with the MLflow evaluate API. You can use this API to evaluate the effectiveness of your \\nembedding model, the top K threshold choice, or the chunking strategy.\\nCREATING A GROUND TRUTH DATASET\\nCurating a ground truth dataset for evaluating your GenAI often involves the meticulous task of manually \\nannotating test sets, a process that demands both time and domain expertise. In this blog, we’re taking a \\ndifferent route. We’re leveraging the power of an LLM to generate synthetic data for testing , offering a quick-\\nstart approach to get a sense of your GenAI app’s retrieval capability, and a warm-up for all the in-depth \\nevaluation work that may follow. To our readers and customers, we emphasize the importance of crafting a \\ndataset that mirrors the expected inputs and outputs of your GenAI application. It’s a journey worth taking for \\nthe incredible insights you’ll gain!\\nYou can explore with the full dataset but let's demo with a subset of the generated data. The question column \\ncontains all the questions that will be evaluated and the source column is the expected source for the answer \\nfor the questions as an ordered list of strings.107\", metadata={'source': 'genai_notes.pdf', 'page': 106}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16eval_data = pd.DataFrame(\\n    {\\n        \"question\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n        \"source\": [\\n            [\"https://mlflow.org/docs/latest/index.html\"],\\n            [\"https://mlflow.org/docs/latest/getting-started/tracking-server-overview/index.html\"],\\n            [\"https://mlflow.org/docs/latest/python_api/mlflow.deployments.html\"],\\n            [\"https://mlflow.org/docs/latest/tracking/autolog.html\"],\\n        ],\\n    }\\n)\\nEVALUATE THE EMBEDDING MODEL WITH MLFLOW\\nThe quality of your embedding model is pivotal for accurate retrieval. In MLflow 2.9.0, we introduced three built-\\nin metrics mlflow.metrics.precision_at_k(k) , mlflow.metrics.recall_at_k(k)  and mlflow.metrics.ndcg_at_k(k)  \\nto help determine how effective your retriever is at predicting the most relevant results for you. For example; \\nSuppose the vector database returns 10 results (k=10), and out of these 10 results, 4 are relevant to your query. \\nThe precision_at_10 would be 4/10 or 40%. 108', metadata={'source': 'genai_notes.pdf', 'page': 107}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AI \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25\\n26def evaluate_embedding(embedding_function):\\n    CHUNK_SIZE = 1000\\n    list_of_documents = loader.load()\\n    text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\\n    docs = text_splitter.split_documents(list_of_documents)\\n    retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n    def retrieve_doc_ids(question: str) -> List[str]:\\n        docs = retriever.get_relevant_documents(question)\\n        doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n        return doc_ids\\n    def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n        return question_df[\"question\"].apply(retrieve_doc_ids)\\n    with mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n                model=retriever_model_function,\\n                data=eval_data,\\n                model_type=\"retriever\",\\n                targets=\"source\",\\n                evaluators=\"default\",\\n            )\\n    return evaluate_results\\nresult1 = evaluate_embedding(DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\"))result2 = evaluate_embed -\\nding(<another-embedding-function>)\\neval_results_of_retriever_df_bge = result1.tables[\"eval_results_table\"]\\ndisplay(eval_results_of_retriever_df_bge)109', metadata={'source': 'genai_notes.pdf', 'page': 108}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe evaluation will return a table with the results of your evaluation for each question. i.e., for this test, we can \\nsee that the retriever seems to performing great for the questions \"How to enable MLflow Autologging for my \\nworkspace by default?” with a Precision @ K score is 1, and is not retrieving any of the right documentation for \\nthe questions \"What is MLflow?” since the precision @ K score is 0. With this insight, we can debug the retriever \\nand improve the retriever for questions like “What is MLflow?”\\nEvaluation results when using databricks-bge-large-en embedding model\\nEVALUATE RETRIEVER WITH DIFFERENT TOP K VALUES WITH MLFLOW\\nYou can quickly calculate the metrics for different Ks by specifying the extra_metrics argument.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19with mlflow.start_run() as run:\\n        evaluate_results = mlflow.evaluate(\\n        data=eval_results_of_retriever_df_bge,\\n        targets=\"source\",\\n        predictions=\"outputs\",\\n        evaluators=\"default\",\\n        extra_metrics=[\\n            mlflow.metrics.precision_at_k(1),\\n            mlflow.metrics.precision_at_k(2),\\n            mlflow.metrics.precision_at_k(3),\\n            mlflow.metrics.recall_at_k(1),\\n            mlflow.metrics.recall_at_k(2),\\n            mlflow.metrics.recall_at_k(3),\\n            mlflow.metrics.ndcg_at_k(1),\\n            mlflow.metrics.ndcg_at_k(2),\\n            mlflow.metrics.ndcg_at_k(3),\\n        ],\\n    )\\ndisplay(evaluate_results.tables[\"eval_results_table\"])110', metadata={'source': 'genai_notes.pdf', 'page': 109}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe evaluation will return a table with the results of your evaluation for each question, and you can better \\nunderstand which K value to use when retrieving documents. i.e., for this test we can see changing the top K \\nvalue can positively affect the precision of the retriever for questions like “What is Databricks?”\\nEvaluation result with all precision at K values\\n111', metadata={'source': 'genai_notes.pdf', 'page': 110}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIEVALUATE THE CHUNKING STRATEGY WITH MLFLOW\\nThe effectiveness of your chunking strategy is critical. We explore how MLflow can assist in this evaluation, \\nfocusing on the retrieval model type and its impact on overall performance.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n25def evaluate_chunk_size(chunk_size):\\n  list_of_documents = loader.load()\\n  text_splitter = CharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=0)\\n  docs = text_splitter.split_documents(list_of_documents)\\n  embedding_function = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\\n  retriever = Chroma.from_documents(docs, embedding_function).as_retriever()\\n  \\n  def retrieve_doc_ids(question: str) -> List[str]:\\n    docs = retriever.get_relevant_documents(question)\\n    doc_ids = [doc.metadata[\"source\"] for doc in docs]\\n    return doc_ids\\n   \\n  def retriever_model_function(question_df: pd.DataFrame) -> pd.Series:\\n    return question_df[\"question\"].apply(retrieve_doc_ids)\\n  with mlflow.start_run() as run:\\n      evaluate_results = mlflow.evaluate(\\n          model=retriever_model_function,\\n          data=eval_data,\\n          model_type=\"retriever\",\\n          targets=\"source\",\\n          evaluators=\"default\",\\n      )\\n  return evaluate_results\\nresult1 = evaluate_chunk_size(500)\\nresult2 = evaluate_chunk_size(2000)\\ndisplay(result1.tables[\"eval_results_table\"])\\ndisplay(result2.tables[\"eval_results_table\"])112', metadata={'source': 'genai_notes.pdf', 'page': 111}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIThe evaluation will return 2 tables with the results of your evaluation for each question using 2 different chunk \\nsizes, and you can better understand which chunk size to use when retrieving documents (i.e., for this example, \\nit seems like changing the chunk size did not affect any metric).\\nEvaluation result with Chunk size of 1000\\nEvaluation result with Chunk size of 2000\\nCheck out the in-depth notebook on retrieval evaluation\\n113', metadata={'source': 'genai_notes.pdf', 'page': 112}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIEVALUATION OF GENAI RESULTS WITH MLFLOW\\nIn this section, we will understand: How good is the response of the GenAI app with a given prompt and context?\\nAssessing the quality of generated responses is key. We will augment the manual process of evaluating with \\nquestions and answers by leveraging MLflow\\'s QA metrics, and comparing them against a GPT-4 model as a \\nbenchmark to understand the effectiveness of the generated answers. \\nUsing an LLM like GPT-4 as a judge to assist in evaluation  can offer several benefits, here are some key benefits:\\n ■Rapid and Scalable Experimentation:  In many situations, we think LLM judges represent a sweet-spot: \\nthey can evaluate unstructured outputs (like a response from a chat-bot) automatically, rapidly, and  \\nat low-cost.  \\n ■Cost-Effective: By automating some evaluations with LLMs, we consider it a worthy companion to human \\nevaluation, which is slower and more expensive but represents the gold standard of model evaluation.\\nUSE MLFLOW EVALUATE AND LLM AS A JUDGE\\nWe take some sample questions and use the LLM as a judge, and inspect the results with MLflow, providing a \\ncomprehensive analysis of the outcome with built-in metrics. We are going to judge the GenAI app on relevance \\n(how relevant is the output with respect to both the input and the context).\\nCreate a simple function that runs each input through the chain\\n \\n1\\n2def model(input_df):\\n    return input_df[\"questions\"].map(qa).tolist()\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10eval_df = pd.DataFrame(\\n    {\\n        \"questions\": [\\n            \"What is MLflow?\",\\n            \"What is Databricks?\",\\n            \"How to serve a model on Databricks?\",\\n            \"How to enable MLflow Autologging for my workspace by default?\",\\n        ],\\n    }\\n)114', metadata={'source': 'genai_notes.pdf', 'page': 113}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIUse relevance metric to determine the relevance of the answer and context. There are other metrics  you can \\nuse too.\\n \\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21from mlflow.deployments import set_deployments_target\\nfrom  mlflow.metrics.genai.metric_definitions import relevance\\nset_deployments_target(\"databricks\") #To retrieve all endpoint in your Databricks Workspace\\nrelevance_metric = relevance(model=f\"endpoints:/{endpoint_name}\") #You can also use any model you have hosted on Da -\\ntabricks, models from the Marketplace or models in the Foundation model API\\nwith mlflow.start_run():\\n    results =  mlflow.evaluate(\\n        model,\\n        eval_df,\\n        model_type=\"question-answering\",\\n        evaluators=\"default\",\\n        predictions=\"result\",\\n        extra_metrics=[relevance_metric, mlflow.metrics.latency()],\\n        evaluator_config={\\n            \"col_mapping\": {\\n                \"inputs\": \"questions\",\\n                \"context\": \"source_documents\",\\n            }\\n        }\\n    )\\n    print(results.metrics)115', metadata={'source': 'genai_notes.pdf', 'page': 114}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AIIn your Databricks Workspace, you can compare and evaluate all your inputs and outputs, as well as the source \\ndocuments, relevance and any other metrics you added to your evaluation function.\\nCheck out more in depth notebooks on LLM evaluation\\n116', metadata={'source': 'genai_notes.pdf', 'page': 115}),\n",
       " Document(page_content='THE BIG BOOK OF GENERATIVE AISummaryWhether you’re looking to disrupt traditional industries, enhance creative endeavors or solve complex problems \\nin novel ways, the potential applications of generative AI are limited only by your imagination and willingness to \\nexperiment. Remember, every significant advancement in this field began with a simple idea and the courage to \\nexplore it further.\\nFor those seeking more knowledge or simply curious about the latest developments in the realm of generative \\nAI, we’ve provided some resources on training, demos and product information. \\nGenAI Training\\nGenerative AI Engineer Learning Pathway : Take self-paced, on-demand and instructor-led courses on \\ngenerative AI\\nFree LLM Course (edX) : In-depth course to learn GenAI and LLMs inside and out\\nGenAI Webinar : Learn how to take control of your GenAI app performance, privacy and cost, and drive value \\nwith generative AI\\nAdditional Resources\\nBig Book of MLOps : A deep dive into the architectures and technologies behind MLOps — including LLMs  \\nand GenAI \\nMosaic AI : Product page covering the features of Mosaic AI within Databricks117', metadata={'source': 'genai_notes.pdf', 'page': 116}),\n",
       " Document(page_content='Build Production-Quality GenAI Applications — See How\\nCreate high-quality generative AI applications and ensure your output is accurate, \\ngoverned and safe. See why over 10,000 organizations worldwide rely on Databricks for \\nall their workloads from BI to AI — test-drive the full Databricks Platform free for 14 days.\\nAbout Databricks\\nDatabricks is the data and AI company. More than 10,000 organizations worldwide — \\nincluding Comcast, Condé Nast, Grammarly and over 50% of the Fortune 500 — rely on \\nthe Databricks Data Intelligence Platform to unify and democratize data, analytics and \\nAI. Databricks is headquartered in San Francisco, with offices around the globe, and was \\nfounded by the original creators of Lakehouse, Apache Spark™, Delta Lake and MLflow.  \\nTo learn more, follow Databricks on LinkedIn , X and Facebook .Try Databricks free Take Generative AI Fundamentals On-Demand Training\\n© Databricks 2024. All rights reserved. Apache, Apache Spark, Spark and the Spark logo are trademarks of the Apache Software Foundation . Privacy Policy  | Terms of Use', metadata={'source': 'genai_notes.pdf', 'page': 117})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('genai_notes.pdf')\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
